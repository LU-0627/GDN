# -*- coding: utf-8 -*-
"""
GDN è°ƒè¯•æ—¥å¿—æ¨¡å—
ç”¨äºåœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­è¾“å‡ºè¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ï¼Œå¸®åŠ©åˆå­¦è€…ç†è§£æ¨¡å‹è¿è¡Œè¿‡ç¨‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    from util.debug_logger import DebugLogger
    logger = DebugLogger(debug=True, log_dir='./logs', dataset_name='msl')
    logger.log_section("æ•°æ®åŠ è½½")
    logger.log("è®­ç»ƒæ ·æœ¬æ•°", 1000)
    logger.log_tensor("è¾“å…¥x", x)
"""

import os
import sys
from datetime import datetime
from pathlib import Path
import torch
import numpy as np


class Colors:
    """ç»ˆç«¯å½©è‰²è¾“å‡º (Windowså…¼å®¹)"""
    HEADER = '\033[95m'      # ç´«è‰²
    BLUE = '\033[94m'        # è“è‰²
    CYAN = '\033[96m'        # é’è‰²
    GREEN = '\033[92m'        # ç»¿è‰²
    YELLOW = '\033[93m'      # é»„è‰²
    RED = '\033[91m'         # çº¢è‰²
    ENDC = '\033[0m'         # ç»“æŸ
    BOLD = '\033[1m'         # ç²—ä½“
    
    @classmethod
    def disable(cls):
        """ç¦ç”¨é¢œè‰²ï¼ˆç”¨äºæ–‡ä»¶è¾“å‡ºï¼‰"""
        cls.HEADER = ''
        cls.BLUE = ''
        cls.CYAN = ''
        cls.GREEN = ''
        cls.YELLOW = ''
        cls.RED = ''
        cls.ENDC = ''
        cls.BOLD = ''


class DebugLogger:
    """
    è°ƒè¯•æ—¥å¿—å™¨
    æ”¯æŒæ§åˆ¶å°å½©è‰²è¾“å‡ºå’Œæ–‡ä»¶è®°å½•
    """
    
    def __init__(self, debug=False, log_dir='./logs', dataset_name='default', 
                 debug_batch=1, debug_forward=False):
        """
        åˆå§‹åŒ–æ—¥å¿—å™¨
        
        Args:
            debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
            log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
            dataset_name: æ•°æ®é›†åç§°ï¼ˆç”¨äºæ—¥å¿—æ–‡ä»¶å‘½åï¼‰
            debug_batch: æ¯Nä¸ªbatchæ‰“å°ä¸€æ¬¡
            debug_forward: æ˜¯å¦æ‰“å°forwardå†…éƒ¨ç»†èŠ‚
        """
        self.debug = debug
        self.debug_batch = debug_batch
        self.debug_forward = debug_forward
        self.indent_level = 0
        self.log_file = None
        self.start_time = datetime.now()
        
        if self.debug:
            # åˆ›å»ºæ—¥å¿—ç›®å½•
            self.log_dir = Path(log_dir)
            self.log_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆå”¯ä¸€çš„æ—¥å¿—æ–‡ä»¶åï¼ˆå¤„ç†é‡åé—®é¢˜ï¼‰
            self.log_filename = self._get_unique_filename(dataset_name)
            self.log_file = open(self.log_filename, 'w', encoding='utf-8')
            
            # Windows å¯ç”¨ANSIé¢œè‰²æ”¯æŒ
            if sys.platform == 'win32':
                os.system('color')
            
            self._print_header()
    
    def _get_unique_filename(self, dataset_name):
        """
        ç”Ÿæˆå”¯ä¸€çš„æ—¥å¿—æ–‡ä»¶åï¼Œé¿å…é‡åè¦†ç›–
        æ ¼å¼: dataset_YYYYMMDD_HHMMSS_åºå·.log
        """
        timestamp = self.start_time.strftime('%Y%m%d_%H%M%S')
        base_name = f"{dataset_name}_{timestamp}"
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåæ–‡ä»¶ï¼Œæ·»åŠ åºå·
        index = 0
        while True:
            if index == 0:
                filename = self.log_dir / f"{base_name}.log"
            else:
                filename = self.log_dir / f"{base_name}_{index}.log"
            
            if not filename.exists():
                return filename
            index += 1
    
    def _print_header(self):
        """æ‰“å°æ—¥å¿—å¤´éƒ¨ä¿¡æ¯"""
        header = f"""
{'â•' * 60}
  GDN è°ƒè¯•æ—¥å¿— - Debug Log
  å¼€å§‹æ—¶é—´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}
  æ—¥å¿—æ–‡ä»¶: {self.log_filename}
{'â•' * 60}
"""
        self._write(header)
    
    def _write(self, message, to_console=True, to_file=True):
        """å†™å…¥æ¶ˆæ¯åˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
        if not self.debug:
            return
            
        if to_console:
            print(message, flush=True)
        
        if to_file and self.log_file:
            # æ–‡ä»¶ä¸­ä¸å†™å…¥é¢œè‰²ä»£ç 
            clean_msg = message
            for attr in ['HEADER', 'BLUE', 'CYAN', 'GREEN', 'YELLOW', 'RED', 'ENDC', 'BOLD']:
                clean_msg = clean_msg.replace(getattr(Colors, attr), '')
            self.log_file.write(clean_msg + '\n')
            self.log_file.flush()
    
    def log_section(self, title, icon='ğŸ“'):
        """
        æ‰“å°åˆ†èŠ‚æ ‡é¢˜
        
        Args:
            title: æ ‡é¢˜æ–‡å­—
            icon: å‰ç½®å›¾æ ‡
        """
        if not self.debug:
            return
        
        msg = f"\n{Colors.BOLD}{Colors.CYAN}{'â•' * 60}{Colors.ENDC}"
        msg += f"\n{Colors.BOLD}{icon} [{title}]{Colors.ENDC}"
        msg += f"\n{Colors.CYAN}{'â•' * 60}{Colors.ENDC}"
        self._write(msg)
        self.indent_level = 0
    
    def log_subsection(self, title, icon='ğŸ“Œ'):
        """æ‰“å°å°èŠ‚æ ‡é¢˜"""
        if not self.debug:
            return
        
        msg = f"\n{Colors.YELLOW}  {icon} {title}{Colors.ENDC}"
        self._write(msg)
    
    def log(self, key, value=None, level=0):
        """
        æ‰“å°é”®å€¼å¯¹æ—¥å¿—
        
        Args:
            key: é”®å
            value: å€¼ï¼ˆå¯é€‰ï¼‰
            level: ç¼©è¿›çº§åˆ«
        """
        if not self.debug:
            return
        
        indent = '  ' * (self.indent_level + level)
        tree_char = 'â”œâ”€' if level > 0 else ''
        
        if value is None:
            msg = f"{indent}{tree_char}{key}"
        else:
            msg = f"{indent}{tree_char}{Colors.GREEN}{key}{Colors.ENDC}: {value}"
        
        self._write(msg)
    
    def log_tensor(self, name, tensor, show_stats=True, level=0):
        """
        æ‰“å°å¼ é‡ä¿¡æ¯
        
        Args:
            name: å¼ é‡åç§°
            tensor: PyTorchå¼ é‡æˆ–Numpyæ•°ç»„
            show_stats: æ˜¯å¦æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            level: ç¼©è¿›çº§åˆ«
        """
        if not self.debug:
            return
        
        indent = '  ' * (self.indent_level + level)
        
        if isinstance(tensor, torch.Tensor):
            shape = list(tensor.shape)
            dtype = str(tensor.dtype).replace('torch.', '')
            device = str(tensor.device)
            
            msg = f"{indent}{Colors.BLUE}{name}{Colors.ENDC}: shape={shape}, dtype={dtype}, device={device}"
            
            if show_stats and tensor.numel() > 0:
                t = tensor.float()
                stats = f" | min={t.min().item():.4f}, max={t.max().item():.4f}, mean={t.mean().item():.4f}"
                msg += stats
                
        elif isinstance(tensor, np.ndarray):
            shape = list(tensor.shape)
            dtype = str(tensor.dtype)
            
            msg = f"{indent}{Colors.BLUE}{name}{Colors.ENDC}: shape={shape}, dtype={dtype}"
            
            if show_stats and tensor.size > 0:
                stats = f" | min={tensor.min():.4f}, max={tensor.max():.4f}, mean={tensor.mean():.4f}"
                msg += stats
        else:
            msg = f"{indent}{Colors.BLUE}{name}{Colors.ENDC}: {type(tensor)}"
        
        self._write(msg)
    
    def log_dict(self, d, title=None, level=0):
        """æ‰“å°å­—å…¸"""
        if not self.debug:
            return
        
        if title:
            self.log(title, level=level)
        
        for key, value in d.items():
            self.log(f"  {key}", value, level=level)
    
    def log_model_summary(self, model):
        """æ‰“å°æ¨¡å‹æ‘˜è¦"""
        if not self.debug:
            return
        
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        self.log_subsection("æ¨¡å‹å‚æ•°ç»Ÿè®¡")
        self.log("æ€»å‚æ•°é‡", f"{total_params:,}")
        self.log("å¯è®­ç»ƒå‚æ•°", f"{trainable_params:,}")
        self.log("ä¸å¯è®­ç»ƒå‚æ•°", f"{total_params - trainable_params:,}")
    
    def log_batch(self, batch_idx, total_batches, data_dict):
        """
        æ‰“å°æ‰¹æ¬¡ä¿¡æ¯
        
        Args:
            batch_idx: å½“å‰æ‰¹æ¬¡ç´¢å¼•
            total_batches: æ€»æ‰¹æ¬¡æ•°
            data_dict: æ•°æ®å­—å…¸ {name: tensor}
        """
        if not self.debug:
            return
        
        # æ ¹æ® debug_batch å†³å®šæ˜¯å¦æ‰“å°
        if (batch_idx + 1) % self.debug_batch != 0:
            return
        
        self.log_subsection(f"Batch {batch_idx + 1}/{total_batches}", icon='ğŸ“¦')
        for name, tensor in data_dict.items():
            self.log_tensor(name, tensor, level=1)
    
    def log_epoch_start(self, epoch, total_epochs):
        """æ‰“å°Epochå¼€å§‹"""
        if not self.debug:
            return
        
        self.log_section(f"Epoch {epoch + 1}/{total_epochs}", icon='ğŸ”„')
    
    def log_epoch_end(self, epoch, train_loss, val_loss=None, best=False, time_elapsed=None):
        """æ‰“å°Epochç»“æŸ"""
        if not self.debug:
            return
        
        self.log_subsection("Epoch å®Œæˆ", icon='âœ…')
        self.log("è®­ç»ƒæŸå¤±", f"{train_loss:.6f}", level=1)
        
        if val_loss is not None:
            status = f"{val_loss:.6f}"
            if best:
                status += f" {Colors.GREEN}âœ“ æ–°æœ€ä¼˜{Colors.ENDC}"
            self.log("éªŒè¯æŸå¤±", status, level=1)
        
        if time_elapsed is not None:
            self.log("è€—æ—¶", f"{time_elapsed:.2f}s", level=1)
    
    def log_training_complete(self, total_epochs, best_loss, model_path):
        """æ‰“å°è®­ç»ƒå®Œæˆ"""
        if not self.debug:
            return
        
        self.log_section("è®­ç»ƒå®Œæˆ", icon='ğŸ‰')
        self.log("æ€»Epochæ•°", total_epochs)
        self.log("æœ€ä½³éªŒè¯æŸå¤±", f"{best_loss:.6f}")
        self.log("æ¨¡å‹ä¿å­˜è·¯å¾„", model_path)
    
    def log_test_start(self):
        """æ‰“å°æµ‹è¯•å¼€å§‹"""
        if not self.debug:
            return
        
        self.log_section("æµ‹è¯•é˜¶æ®µ", icon='ğŸ§ª')
    
    def log_test_complete(self, avg_loss, total_samples, pred_stats, gt_stats):
        """æ‰“å°æµ‹è¯•å®Œæˆ"""
        if not self.debug:
            return
        
        self.log_subsection("æµ‹è¯•å®Œæˆ", icon='âœ…')
        self.log("æ€»æ ·æœ¬æ•°", total_samples, level=1)
        self.log("å¹³å‡æŸå¤±", f"{avg_loss:.6f}", level=1)
        self.log("é¢„æµ‹å€¼", f"min={pred_stats[0]:.4f}, max={pred_stats[1]:.4f}, mean={pred_stats[2]:.4f}", level=1)
        self.log("çœŸå®å€¼", f"min={gt_stats[0]:.4f}, max={gt_stats[1]:.4f}, mean={gt_stats[2]:.4f}", level=1)
    
    def log_evaluation_result(self, metrics):
        """
        æ‰“å°è¯„ä¼°ç»“æœ
        
        Args:
            metrics: æŒ‡æ ‡å­—å…¸ {'f1': ..., 'precision': ..., 'recall': ..., 'auc': ..., 'threshold': ...}
        """
        if not self.debug:
            return
        
        self.log_section("è¯„ä¼°ç»“æœ", icon='ğŸ†')
        
        # æ‰“å°è¡¨æ ¼
        line = f"\n{'â”€' * 40}"
        header = f"â”‚ {'æŒ‡æ ‡':<12} â”‚ {'æ•°å€¼':<20} â”‚"
        self._write(line)
        self._write(header)
        self._write(line)
        
        for key, value in metrics.items():
            if isinstance(value, float):
                row = f"â”‚ {key:<12} â”‚ {value:<20.4f} â”‚"
            else:
                row = f"â”‚ {key:<12} â”‚ {str(value):<20} â”‚"
            self._write(row)
        
        self._write(line)
    
    def log_forward_step(self, step_name, tensor=None, extra_info=None):
        """æ‰“å°Forwardè¿‡ç¨‹ä¸­çš„æ­¥éª¤"""
        if not self.debug or not self.debug_forward:
            return
        
        msg = f"    â”œâ”€ {step_name}"
        if tensor is not None:
            if isinstance(tensor, torch.Tensor):
                msg += f": {list(tensor.shape)}"
        if extra_info:
            msg += f" ({extra_info})"
        
        self._write(msg)
    
    def log_loss(self, loss, pred_range=None, gt_range=None):
        """æ‰“å°æŸå¤±ä¿¡æ¯"""
        if not self.debug:
            return
        
        self.log_subsection("æŸå¤±è®¡ç®—", icon='ğŸ“‰')
        if pred_range:
            self.log("é¢„æµ‹å€¼èŒƒå›´", f"[{pred_range[0]:.4f}, {pred_range[1]:.4f}]", level=1)
        if gt_range:
            self.log("çœŸå®å€¼èŒƒå›´", f"[{gt_range[0]:.4f}, {gt_range[1]:.4f}]", level=1)
        self.log("MSE Loss", f"{loss:.6f}", level=1)
    
    def log_gradient(self, model):
        """æ‰“å°æ¢¯åº¦ä¿¡æ¯"""
        if not self.debug or not self.debug_forward:
            return
        
        total_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                total_norm += p.grad.data.norm(2).item() ** 2
        total_norm = total_norm ** 0.5
        
        self.log_subsection("åå‘ä¼ æ’­", icon='ğŸ“ˆ')
        self.log("æ¢¯åº¦èŒƒæ•°", f"{total_norm:.4f}", level=1)
    
    def close(self):
        """å…³é—­æ—¥å¿—æ–‡ä»¶"""
        if self.log_file:
            end_time = datetime.now()
            duration = (end_time - self.start_time).total_seconds()
            
            footer = f"""
{'â•' * 60}
  æ—¥å¿—ç»“æŸ
  ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
  æ€»è€—æ—¶: {duration:.2f}s
{'â•' * 60}
"""
            self._write(footer)
            self.log_file.close()
            self.log_file = None
    
    def __del__(self):
        """ææ„æ—¶å…³é—­æ–‡ä»¶"""
        self.close()


# å…¨å±€æ—¥å¿—å®ä¾‹ï¼ˆæ–¹ä¾¿åœ¨å„æ¨¡å—ä¸­ä½¿ç”¨ï¼‰
_global_logger = None

def init_global_logger(**kwargs):
    """åˆå§‹åŒ–å…¨å±€æ—¥å¿—å™¨"""
    global _global_logger
    _global_logger = DebugLogger(**kwargs)
    return _global_logger

def get_logger():
    """è·å–å…¨å±€æ—¥å¿—å™¨"""
    global _global_logger
    if _global_logger is None:
        _global_logger = DebugLogger(debug=False)
    return _global_logger

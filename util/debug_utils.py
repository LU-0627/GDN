# -*- coding: utf-8 -*-
"""
è°ƒè¯•å·¥å…·å‡½æ•°é›†åˆ
æä¾›å¸¸ç”¨çš„è°ƒè¯•åŠŸèƒ½ï¼Œå¦‚å¼ é‡ç»Ÿè®¡ã€æ•°æ®å¯è§†åŒ–ç­‰
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path


def print_tensor_stats(tensor, name="Tensor", show_values=False):
    """
    æ‰“å°å¼ é‡çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        tensor: PyTorchå¼ é‡
        name: å¼ é‡åç§°
        show_values: æ˜¯å¦æ˜¾ç¤ºéƒ¨åˆ†å€¼
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {name} ç»Ÿè®¡ä¿¡æ¯")
    print(f"{'='*60}")
    print(f"  Shape:        {tensor.shape}")
    print(f"  Dtype:        {tensor.dtype}")
    print(f"  Device:       {tensor.device}")
    print(f"  Range:        [{tensor.min():.4f}, {tensor.max():.4f}]")
    print(f"  Mean:         {tensor.mean():.4f}")
    print(f"  Std:          {tensor.std():.4f}")
    print(f"  Has NaN:      {torch.isnan(tensor).any()}")
    print(f"  Has Inf:      {torch.isinf(tensor).any()}")
    
    if show_values and tensor.numel() <= 100:
        print(f"\n  Values:\n{tensor}")
    elif show_values:
        print(f"\n  First 5 values: {tensor.flatten()[:5]}")
        print(f"  Last 5 values:  {tensor.flatten()[-5:]}")
    print(f"{'='*60}\n")


def compare_tensors(tensor1, tensor2, name1="Tensor1", name2="Tensor2"):
    """
    æ¯”è¾ƒä¸¤ä¸ªå¼ é‡çš„å·®å¼‚
    
    Args:
        tensor1, tensor2: è¦æ¯”è¾ƒçš„å¼ é‡
        name1, name2: å¼ é‡åç§°
    """
    print(f"\n{'='*60}")
    print(f"ğŸ”„ æ¯”è¾ƒ {name1} vs {name2}")
    print(f"{'='*60}")
    
    if tensor1.shape != tensor2.shape:
        print(f"  âš ï¸ Shapeä¸åŒ: {tensor1.shape} vs {tensor2.shape}")
        return
    
    diff = (tensor1 - tensor2).abs()
    print(f"  å¹³å‡ç»å¯¹è¯¯å·®:    {diff.mean():.6f}")
    print(f"  æœ€å¤§ç»å¯¹è¯¯å·®:    {diff.max():.6f}")
    print(f"  ç›¸å¯¹è¯¯å·® (%):    {(diff / (tensor1.abs() + 1e-8)).mean() * 100:.2f}%")
    print(f"  ç›¸åŒå…ƒç´ æ¯”ä¾‹:    {(tensor1 == tensor2).float().mean() * 100:.2f}%")
    print(f"{'='*60}\n")


def plot_batch_distribution(tensor, title="", save_dir="./debug_plots"):
    """
    å¯è§†åŒ–batchä¸­æ•°æ®çš„åˆ†å¸ƒ
    
    Args:
        tensor: è¾“å…¥å¼ é‡ [batch, sensors, time] æˆ– [batch, sensors]
        title: å›¾è¡¨æ ‡é¢˜
        save_dir: ä¿å­˜ç›®å½•
    """
    Path(save_dir).mkdir(parents=True, exist_ok=True)
    
    # è½¬æ¢ä¸ºnumpy
    data = tensor.cpu().detach().numpy()
    
    plt.figure(figsize=(15, 5))
    
    # å­å›¾1: ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ•°æ®
    plt.subplot(1, 3, 1)
    if len(data.shape) == 3:  # [batch, sensors, time]
        plt.plot(data[0, :, :].T)
        plt.xlabel("ä¼ æ„Ÿå™¨ç´¢å¼•")
        plt.ylabel("å€¼")
        plt.title(f"{title} - ç¬¬ä¸€ä¸ªæ ·æœ¬ (æ‰€æœ‰æ—¶é—´æ­¥)")
    else:  # [batch, sensors]
        plt.bar(range(len(data[0])), data[0])
        plt.xlabel("ä¼ æ„Ÿå™¨ç´¢å¼•")
        plt.ylabel("å€¼")
        plt.title(f"{title} - ç¬¬ä¸€ä¸ªæ ·æœ¬")
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2: æ‰€æœ‰æ ·æœ¬çš„å‡å€¼åˆ†å¸ƒ
    plt.subplot(1, 3, 2)
    if len(data.shape) == 3:
        mean_values = data.mean(axis=(1, 2))
    else:
        mean_values = data.mean(axis=1)
    plt.hist(mean_values, bins=30, edgecolor='black', alpha=0.7)
    plt.xlabel("å‡å€¼")
    plt.ylabel("æ ·æœ¬æ•°é‡")
    plt.title(f"{title} - æ‰¹æ¬¡å‡å€¼åˆ†å¸ƒ")
    plt.grid(True, alpha=0.3)
    
    # å­å›¾3: çƒ­åŠ›å›¾ (ä¼ æ„Ÿå™¨ Ã— æ‰¹æ¬¡)
    plt.subplot(1, 3, 3)
    if len(data.shape) == 3:
        heatmap_data = data.mean(axis=2)  # å¯¹æ—¶é—´ç»´åº¦å–å¹³å‡
    else:
        heatmap_data = data
    plt.imshow(heatmap_data.T, aspect='auto', cmap='viridis')
    plt.colorbar(label='å€¼')
    plt.xlabel("æ‰¹æ¬¡ç´¢å¼•")
    plt.ylabel("ä¼ æ„Ÿå™¨ç´¢å¼•")
    plt.title(f"{title} - çƒ­åŠ›å›¾")
    
    plt.tight_layout()
    filepath = Path(save_dir) / f"{title.replace(' ', '_')}.png"
    plt.savefig(filepath, dpi=100, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“Š å·²ä¿å­˜å¯è§†åŒ–å›¾ç‰‡: {filepath}")


def plot_loss_history(loss_list, title="è®­ç»ƒæŸå¤±æ›²çº¿", save_dir="./debug_plots"):
    """
    ç»˜åˆ¶æŸå¤±æ›²çº¿
    
    Args:
        loss_list: æŸå¤±å€¼åˆ—è¡¨
        title: å›¾è¡¨æ ‡é¢˜
        save_dir: ä¿å­˜ç›®å½•
    """
    Path(save_dir).mkdir(parents=True, exist_ok=True)
    
    plt.figure(figsize=(10, 6))
    plt.plot(loss_list, linewidth=1.5)
    plt.xlabel("è¿­ä»£æ¬¡æ•°")
    plt.ylabel("æŸå¤±å€¼")
    plt.title(title)
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ ç§»åŠ¨å¹³å‡çº¿
    if len(loss_list) > 50:
        window = 50
        moving_avg = np.convolve(loss_list, np.ones(window)/window, mode='valid')
        plt.plot(range(window-1, len(loss_list)), moving_avg, 
                'r--', linewidth=2, label=f'{window}æ­¥ç§»åŠ¨å¹³å‡')
        plt.legend()
    
    filepath = Path(save_dir) / f"{title.replace(' ', '_')}.png"
    plt.savefig(filepath, dpi=100, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“ˆ å·²ä¿å­˜æŸå¤±æ›²çº¿: {filepath}")


def check_gradients(model, threshold=10.0):
    """
    æ£€æŸ¥æ¨¡å‹æ¢¯åº¦æ˜¯å¦æ­£å¸¸
    
    Args:
        model: PyTorchæ¨¡å‹
        threshold: æ¢¯åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼ä¼šå‘å‡ºè­¦å‘Š
    
    Returns:
        grad_info: æ¢¯åº¦ä¿¡æ¯å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” æ¢¯åº¦æ£€æŸ¥")
    print(f"{'='*60}")
    
    total_params = 0
    total_grad_norm = 0
    max_grad = 0
    min_grad = float('inf')
    
    grad_info = {
        'has_nan': False,
        'has_inf': False,
        'exploding': False,
        'vanishing': False
    }
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            total_params += param.numel()
            grad = param.grad
            grad_norm = grad.norm().item()
            total_grad_norm += grad_norm
            
            max_grad = max(max_grad, grad.abs().max().item())
            min_grad = min(min_grad, grad.abs().min().item())
            
            # æ£€æŸ¥å¼‚å¸¸
            if torch.isnan(grad).any():
                print(f"  âš ï¸ {name}: åŒ…å«NaNæ¢¯åº¦")
                grad_info['has_nan'] = True
            
            if torch.isinf(grad).any():
                print(f"  âš ï¸ {name}: åŒ…å«Infæ¢¯åº¦")
                grad_info['has_inf'] = True
            
            if grad_norm > threshold:
                print(f"  âš ï¸ {name}: æ¢¯åº¦è¿‡å¤§ (norm={grad_norm:.2f})")
                grad_info['exploding'] = True
            
            if grad_norm < 1e-7:
                print(f"  âš ï¸ {name}: æ¢¯åº¦è¿‡å° (norm={grad_norm:.2e})")
                grad_info['vanishing'] = True
    
    avg_grad_norm = total_grad_norm / len(list(model.parameters()))
    
    print(f"\n  æ€»å‚æ•°æ•°é‡:      {total_params:,}")
    print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°:    {avg_grad_norm:.6f}")
    print(f"  æœ€å¤§æ¢¯åº¦å€¼:      {max_grad:.6f}")
    print(f"  æœ€å°æ¢¯åº¦å€¼:      {min_grad:.6e}")
    print(f"{'='*60}\n")
    
    return grad_info


def watch_variable(var, var_name="variable", epoch=None, batch=None, log_file="./debug_watch.log"):
    """
    æŒç»­ç›‘æ§æŸä¸ªå˜é‡çš„å˜åŒ–ï¼Œå¹¶è®°å½•åˆ°æ–‡ä»¶
    
    Args:
        var: è¦ç›‘æ§çš„å˜é‡ï¼ˆæ”¯æŒTensorã€æ•°å€¼ç­‰ï¼‰
        var_name: å˜é‡å
        epoch: å½“å‰epoch
        batch: å½“å‰batch
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    import datetime
    
    # å‡†å¤‡æ—¥å¿—ä¿¡æ¯
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    location = f"Epoch {epoch}, Batch {batch}" if epoch is not None else ""
    
    # è½¬æ¢å˜é‡ä¸ºå­—ç¬¦ä¸²
    if isinstance(var, torch.Tensor):
        var_str = f"Tensor(shape={var.shape}, mean={var.mean():.4f}, std={var.std():.4f})"
    else:
        var_str = str(var)
    
    # å†™å…¥æ—¥å¿—
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"[{timestamp}] {location} - {var_name}: {var_str}\n")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("è°ƒè¯•å·¥å…·æµ‹è¯•")
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    x = torch.randn(32, 38, 15)
    y = torch.randn(32, 38)
    
    # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯æ‰“å°
    print_tensor_stats(x, "æµ‹è¯•è¾“å…¥å¼ é‡x", show_values=True)
    print_tensor_stats(y, "æµ‹è¯•è¾“å‡ºå¼ é‡y")
    
    # æµ‹è¯•å¯è§†åŒ–
    plot_batch_distribution(x, "æµ‹è¯•è¾“å…¥åˆ†å¸ƒ")
    plot_batch_distribution(y, "æµ‹è¯•è¾“å‡ºåˆ†å¸ƒ")
    
    # æµ‹è¯•æŸå¤±æ›²çº¿
    loss_list = [0.5 * (0.95 ** i) + 0.01 * np.random.randn() for i in range(1000)]
    plot_loss_history(loss_list, "æµ‹è¯•æŸå¤±æ›²çº¿")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

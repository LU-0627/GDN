"""
ç›´æŽ¥æŸ¥çœ‹GDNæ¨¡åž‹çš„åŽŸå§‹åµŒå…¥å‘é‡
æ”¯æŒæ‰“å°ã€ä¿å­˜ã€å¯è§†åŒ–ç­‰å¤šç§æ–¹å¼
"""

import torch
import numpy as np
import os
import argparse
from models.GDN import GDN
from util.env import get_device


def load_model(model_path, node_num=27, dim=64, input_dim=15, topk=20):
    """
    åŠ è½½å·²è®­ç»ƒçš„GDNæ¨¡åž‹
    
    Args:
        model_path: æ¨¡åž‹æƒé‡æ–‡ä»¶è·¯å¾„
        node_num: èŠ‚ç‚¹æ•°é‡
        dim: åµŒå…¥ç»´åº¦
        input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
        topk: TopKå‚æ•°
        
    Returns:
        model: åŠ è½½äº†æƒé‡çš„æ¨¡åž‹
    """
    device = get_device()
    
    # åˆ›å»ºç®€å•çš„è¾¹ç´¢å¼•
    edge_index = torch.zeros((2, node_num * topk), dtype=torch.long)
    edge_index_sets = [edge_index]
    
    # åˆå§‹åŒ–æ¨¡åž‹
    model = GDN(
        edge_index_sets=edge_index_sets,
        node_num=node_num,
        dim=dim,
        input_dim=input_dim,
        topk=topk
    ).to(device)
    
    # åŠ è½½æƒé‡
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"âœ“ æˆåŠŸåŠ è½½æ¨¡åž‹æƒé‡: {model_path}")
    else:
        raise FileNotFoundError(f"æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    return model


def view_embeddings_basic(model):
    """
    åŸºæœ¬æ–¹å¼:æ‰“å°åµŒå…¥å‘é‡çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        model: GDNæ¨¡åž‹å®žä¾‹
    """
    model.eval()
    
    with torch.no_grad():
        embeddings = model.embedding.weight  # [èŠ‚ç‚¹æ•°, åµŒå…¥ç»´åº¦]
        
        print("\n" + "="*80)
        print("ðŸ“Š åµŒå…¥å‘é‡åŸºæœ¬ä¿¡æ¯")
        print("="*80)
        
        print(f"\nå½¢çŠ¶: {embeddings.shape}")
        print(f"  - èŠ‚ç‚¹æ•°é‡: {embeddings.shape[0]}")
        print(f"  - åµŒå…¥ç»´åº¦: {embeddings.shape[1]}")
        print(f"  - æ€»å‚æ•°é‡: {embeddings.numel()}")
        
        print(f"\næ•°å€¼ç»Ÿè®¡:")
        print(f"  - æœ€å°å€¼: {embeddings.min().item():.6f}")
        print(f"  - æœ€å¤§å€¼: {embeddings.max().item():.6f}")
        print(f"  - å¹³å‡å€¼: {embeddings.mean().item():.6f}")
        print(f"  - æ ‡å‡†å·®: {embeddings.std().item():.6f}")
        
        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„L2èŒƒæ•°
        norms = embeddings.norm(dim=1)
        print(f"\nL2èŒƒæ•°ç»Ÿè®¡:")
        print(f"  - æœ€å°L2èŒƒæ•°: {norms.min().item():.6f}")
        print(f"  - æœ€å¤§L2èŒƒæ•°: {norms.max().item():.6f}")
        print(f"  - å¹³å‡L2èŒƒæ•°: {norms.mean().item():.6f}")
        
        print("="*80)


def view_embeddings_detailed(model, num_nodes=5, num_dims=10):
    """
    è¯¦ç»†æ–¹å¼:æ‰“å°å…·ä½“çš„åµŒå…¥å‘é‡å€¼
    
    Args:
        model: GDNæ¨¡åž‹å®žä¾‹
        num_nodes: æ˜¾ç¤ºå‰Nä¸ªèŠ‚ç‚¹
        num_dims: æ˜¾ç¤ºå‰Mä¸ªç»´åº¦
    """
    model.eval()
    
    with torch.no_grad():
        embeddings = model.embedding.weight.cpu().numpy()
        
        print("\n" + "="*80)
        print(f"ðŸ“‹ å‰{num_nodes}ä¸ªèŠ‚ç‚¹çš„åµŒå…¥å‘é‡(å‰{num_dims}ç»´)")
        print("="*80)
        
        for i in range(min(num_nodes, embeddings.shape[0])):
            print(f"\nNode {i:2d}:")
            print(f"  å®Œæ•´å‘é‡å½¢çŠ¶: {embeddings[i].shape}")
            print(f"  å‰{num_dims}ç»´: ", end="")
            
            # æ‰“å°å‰num_dimsä¸ªå€¼
            dims_to_show = min(num_dims, embeddings.shape[1])
            for j in range(dims_to_show):
                print(f"{embeddings[i, j]:8.4f}", end=" ")
            
            if embeddings.shape[1] > num_dims:
                print("...")
            else:
                print()
            
            print(f"  L2èŒƒæ•°: {np.linalg.norm(embeddings[i]):.6f}")
        
        print("="*80)


def view_embeddings_full_matrix(model, num_nodes=None):
    """
    å®Œæ•´çŸ©é˜µæ–¹å¼:æ‰“å°å®Œæ•´çš„åµŒå…¥çŸ©é˜µ
    
    Args:
        model: GDNæ¨¡åž‹å®žä¾‹
        num_nodes: æ˜¾ç¤ºå‰Nä¸ªèŠ‚ç‚¹,Noneè¡¨ç¤ºå…¨éƒ¨
    """
    model.eval()
    
    with torch.no_grad():
        embeddings = model.embedding.weight.cpu().numpy()
        
        if num_nodes is None:
            num_nodes = embeddings.shape[0]
        
        print("\n" + "="*80)
        print(f"ðŸ“Š å®Œæ•´åµŒå…¥çŸ©é˜µ (å‰{num_nodes}ä¸ªèŠ‚ç‚¹)")
        print("="*80)
        
        # è®¾ç½®numpyæ‰“å°é€‰é¡¹
        np.set_printoptions(precision=4, suppress=True, linewidth=200)
        
        print(f"\nå½¢çŠ¶: [{num_nodes}, {embeddings.shape[1]}]")
        print("\nåµŒå…¥çŸ©é˜µ:")
        print(embeddings[:num_nodes])
        
        # æ¢å¤é»˜è®¤æ‰“å°é€‰é¡¹
        np.set_printoptions()
        
        print("\n" + "="*80)


def save_embeddings(model, save_path='embeddings.npy', format='npy'):
    """
    ä¿å­˜åµŒå…¥å‘é‡åˆ°æ–‡ä»¶
    
    Args:
        model: GDNæ¨¡åž‹å®žä¾‹
        save_path: ä¿å­˜è·¯å¾„
        format: ä¿å­˜æ ¼å¼ ('npy', 'txt', 'csv')
    """
    model.eval()
    
    with torch.no_grad():
        embeddings = model.embedding.weight.cpu().numpy()
        
        if format == 'npy':
            np.save(save_path, embeddings)
            print(f"âœ“ åµŒå…¥å‘é‡å·²ä¿å­˜ä¸ºnumpyæ ¼å¼: {save_path}")
            
        elif format == 'txt':
            np.savetxt(save_path, embeddings, fmt='%.6f', delimiter=' ')
            print(f"âœ“ åµŒå…¥å‘é‡å·²ä¿å­˜ä¸ºæ–‡æœ¬æ ¼å¼: {save_path}")
            
        elif format == 'csv':
            np.savetxt(save_path, embeddings, fmt='%.6f', delimiter=',')
            print(f"âœ“ åµŒå…¥å‘é‡å·²ä¿å­˜ä¸ºCSVæ ¼å¼: {save_path}")
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
        
        print(f"  - å½¢çŠ¶: {embeddings.shape}")
        print(f"  - æ–‡ä»¶å¤§å°: {os.path.getsize(save_path) / 1024:.2f} KB")


def visualize_embeddings_heatmap(model, save_path='embeddings_heatmap.png'):
    """
    å¯è§†åŒ–åµŒå…¥å‘é‡çƒ­åŠ›å›¾
    
    Args:
        model: GDNæ¨¡åž‹å®žä¾‹
        save_path: ä¿å­˜è·¯å¾„
    """
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… matplotlib å’Œ seaborn æ‰èƒ½å¯è§†åŒ–")
        print("   è¿è¡Œ: pip install matplotlib seaborn")
        return
    
    model.eval()
    
    with torch.no_grad():
        embeddings = model.embedding.weight.cpu().numpy()
        
        plt.figure(figsize=(14, 8))
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(
            embeddings,
            cmap='RdBu_r',
            center=0,
            cbar_kws={"label": "åµŒå…¥å€¼"},
            xticklabels=10,  # æ¯10ä¸ªç»´åº¦æ˜¾ç¤ºä¸€ä¸ªæ ‡ç­¾
            yticklabels=True
        )
        
        plt.title(f'èŠ‚ç‚¹åµŒå…¥å‘é‡çƒ­åŠ›å›¾\nå½¢çŠ¶: {embeddings.shape}', fontsize=14)
        plt.xlabel('åµŒå…¥ç»´åº¦', fontsize=12)
        plt.ylabel('èŠ‚ç‚¹ç´¢å¼•', fontsize=12)
        plt.tight_layout()
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nðŸ’¾ åµŒå…¥å‘é‡çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")
        
        plt.close()


def analyze_embedding_dimensions(model, top_k=10):
    """
    åˆ†æžåµŒå…¥å‘é‡çš„é‡è¦ç»´åº¦
    
    Args:
        model: GDNæ¨¡åž‹å®žä¾‹
        top_k: æ˜¾ç¤ºå‰Kä¸ªæœ€é‡è¦çš„ç»´åº¦
    """
    model.eval()
    
    with torch.no_grad():
        embeddings = model.embedding.weight.cpu().numpy()
        
        print("\n" + "="*80)
        print("ðŸ” åµŒå…¥ç»´åº¦é‡è¦æ€§åˆ†æž")
        print("="*80)
        
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ–¹å·®(æ–¹å·®è¶Šå¤§,è¯¥ç»´åº¦è¶Šé‡è¦)
        dim_variance = np.var(embeddings, axis=0)
        
        # æ‰¾åˆ°æ–¹å·®æœ€å¤§çš„ç»´åº¦
        top_dims = np.argsort(dim_variance)[::-1][:top_k]
        
        print(f"\nå‰{top_k}ä¸ªæœ€é‡è¦çš„ç»´åº¦(æŒ‰æ–¹å·®æŽ’åº):")
        print("-" * 80)
        for rank, dim in enumerate(top_dims):
            print(f"{rank+1:2d}. ç»´åº¦ {dim:3d}  |  æ–¹å·®: {dim_variance[dim]:.6f}  |  "
                  f"èŒƒå›´: [{embeddings[:, dim].min():.4f}, {embeddings[:, dim].max():.4f}]")
        
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„å¹³å‡ç»å¯¹å€¼
        dim_mean_abs = np.mean(np.abs(embeddings), axis=0)
        top_dims_abs = np.argsort(dim_mean_abs)[::-1][:top_k]
        
        print(f"\nå‰{top_k}ä¸ªå¹³å‡ç»å¯¹å€¼æœ€å¤§çš„ç»´åº¦:")
        print("-" * 80)
        for rank, dim in enumerate(top_dims_abs):
            print(f"{rank+1:2d}. ç»´åº¦ {dim:3d}  |  å¹³å‡ç»å¯¹å€¼: {dim_mean_abs[dim]:.6f}")
        
        print("="*80)


def compare_node_embeddings(model, node_ids):
    """
    æ¯”è¾ƒç‰¹å®šèŠ‚ç‚¹çš„åµŒå…¥å‘é‡
    
    Args:
        model: GDNæ¨¡åž‹å®žä¾‹
        node_ids: è¦æ¯”è¾ƒçš„èŠ‚ç‚¹IDåˆ—è¡¨
    """
    model.eval()
    
    with torch.no_grad():
        embeddings = model.embedding.weight.cpu()
        
        print("\n" + "="*80)
        print(f"ðŸ”„ æ¯”è¾ƒèŠ‚ç‚¹åµŒå…¥å‘é‡: {node_ids}")
        print("="*80)
        
        for i, node_id in enumerate(node_ids):
            if node_id >= embeddings.shape[0]:
                print(f"âš ï¸ èŠ‚ç‚¹ {node_id} è¶…å‡ºèŒƒå›´(æœ€å¤§: {embeddings.shape[0]-1})")
                continue
            
            vec = embeddings[node_id]
            print(f"\nNode {node_id}:")
            print(f"  L2èŒƒæ•°: {vec.norm().item():.6f}")
            print(f"  å¹³å‡å€¼: {vec.mean().item():.6f}")
            print(f"  æ ‡å‡†å·®: {vec.std().item():.6f}")
        
        # è®¡ç®—èŠ‚ç‚¹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        if len(node_ids) >= 2:
            print("\nèŠ‚ç‚¹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦:")
            print("-" * 60)
            from torch.nn.functional import cosine_similarity
            
            for i in range(len(node_ids)):
                for j in range(i+1, len(node_ids)):
                    if node_ids[i] < embeddings.shape[0] and node_ids[j] < embeddings.shape[0]:
                        sim = cosine_similarity(
                            embeddings[node_ids[i]].unsqueeze(0),
                            embeddings[node_ids[j]].unsqueeze(0)
                        )
                        print(f"  Node {node_ids[i]:2d} â†” Node {node_ids[j]:2d}: {sim.item():.6f}")
        
        print("="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æŸ¥çœ‹GDNæ¨¡åž‹çš„åŽŸå§‹åµŒå…¥å‘é‡')
    parser.add_argument('--model_path', type=str, 
                        default='pretrained/msl/best_01_07-154250.pt',
                        help='æ¨¡åž‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--node_num', type=int, default=27,
                        help='èŠ‚ç‚¹æ•°é‡(é»˜è®¤: 27)')
    parser.add_argument('--dim', type=int, default=64,
                        help='åµŒå…¥ç»´åº¦(é»˜è®¤: 64)')
    parser.add_argument('--input_dim', type=int, default=15,
                        help='è¾“å…¥ç‰¹å¾ç»´åº¦(é»˜è®¤: 15)')
    parser.add_argument('--topk', type=int, default=20,
                        help='TopKå‚æ•°(é»˜è®¤: 20)')
    
    # æ˜¾ç¤ºé€‰é¡¹
    parser.add_argument('--basic', action='store_true', default=True,
                        help='æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--detailed', action='store_true',
                        help='æ˜¾ç¤ºè¯¦ç»†çš„åµŒå…¥å€¼')
    parser.add_argument('--full', action='store_true',
                        help='æ˜¾ç¤ºå®Œæ•´çŸ©é˜µ')
    parser.add_argument('--num_nodes', type=int, default=5,
                        help='è¯¦ç»†æ¨¡å¼ä¸‹æ˜¾ç¤ºçš„èŠ‚ç‚¹æ•°(é»˜è®¤: 5)')
    parser.add_argument('--num_dims', type=int, default=10,
                        help='è¯¦ç»†æ¨¡å¼ä¸‹æ˜¾ç¤ºçš„ç»´åº¦æ•°(é»˜è®¤: 10)')
    
    # åˆ†æžé€‰é¡¹
    parser.add_argument('--analyze_dims', action='store_true',
                        help='åˆ†æžç»´åº¦é‡è¦æ€§')
    parser.add_argument('--compare_nodes', type=int, nargs='+',
                        help='æ¯”è¾ƒç‰¹å®šèŠ‚ç‚¹,ä¾‹å¦‚: --compare_nodes 0 1 5')
    
    # ä¿å­˜é€‰é¡¹
    parser.add_argument('--save', action='store_true',
                        help='ä¿å­˜åµŒå…¥å‘é‡')
    parser.add_argument('--save_path', type=str, default='embeddings.npy',
                        help='ä¿å­˜è·¯å¾„(é»˜è®¤: embeddings.npy)')
    parser.add_argument('--format', type=str, default='npy',
                        choices=['npy', 'txt', 'csv'],
                        help='ä¿å­˜æ ¼å¼')
    
    # å¯è§†åŒ–é€‰é¡¹
    parser.add_argument('--visualize', action='store_true',
                        help='ç”Ÿæˆçƒ­åŠ›å›¾')
    parser.add_argument('--fig_path', type=str, default='embeddings_heatmap.png',
                        help='çƒ­åŠ›å›¾ä¿å­˜è·¯å¾„')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("GDNæ¨¡åž‹ - åŽŸå§‹åµŒå…¥å‘é‡æŸ¥çœ‹å™¨")
    print("="*80)
    
    # åŠ è½½æ¨¡åž‹
    print(f"\nðŸ“‚ åŠ è½½æ¨¡åž‹: {args.model_path}")
    model = load_model(
        model_path=args.model_path,
        node_num=args.node_num,
        dim=args.dim,
        input_dim=args.input_dim,
        topk=args.topk
    )
    
    # åŸºæœ¬ä¿¡æ¯
    if args.basic or not any([args.detailed, args.full, args.analyze_dims, args.compare_nodes]):
        view_embeddings_basic(model)
    
    # è¯¦ç»†ä¿¡æ¯
    if args.detailed:
        view_embeddings_detailed(model, args.num_nodes, args.num_dims)
    
    # å®Œæ•´çŸ©é˜µ
    if args.full:
        view_embeddings_full_matrix(model, args.num_nodes)
    
    # ç»´åº¦åˆ†æž
    if args.analyze_dims:
        analyze_embedding_dimensions(model)
    
    # èŠ‚ç‚¹æ¯”è¾ƒ
    if args.compare_nodes:
        compare_node_embeddings(model, args.compare_nodes)
    
    # ä¿å­˜
    if args.save:
        save_embeddings(model, args.save_path, args.format)
    
    # å¯è§†åŒ–
    if args.visualize:
        visualize_embeddings_heatmap(model, args.fig_path)
    
    print("\nâœ“ æŸ¥çœ‹å®Œæˆ!\n")


if __name__ == '__main__':
    main()

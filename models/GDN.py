"""
GDN (Graph Deviation Network) æ¨¡å‹
ç”¨äºå¤šå˜é‡æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹çš„å›¾ç¥ç»ç½‘ç»œ

ä¸»è¦ç»„ä»¶:
1. OutLayer: è¾“å‡ºå±‚MLP
2. GNNLayer: å›¾ç¥ç»ç½‘ç»œå±‚ï¼ˆåŒ…å«GraphLayer + BatchNorm + ReLUï¼‰
3. GDN: ä¸»æ¨¡å‹ï¼ŒåŒ…å«èŠ‚ç‚¹åµŒå…¥ã€å›¾ç»“æ„å­¦ä¹ ã€GNNå±‚å’Œè¾“å‡ºå±‚
"""
import numpy as np  # å¯¼å…¥numpyåº“ï¼Œç”¨äºæ•°å€¼è®¡ç®—
import torch  # å¯¼å…¥PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import matplotlib.pyplot as plt  # å¯¼å…¥matplotlibç»˜å›¾åº“
import torch.nn as nn  # å¯¼å…¥PyTorchç¥ç»ç½‘ç»œæ¨¡å—
import time  # å¯¼å…¥æ—¶é—´å¤„ç†æ¨¡å—
import math  # å¯¼å…¥æ•°å­¦è¿ç®—æ¨¡å—
import torch.nn.functional as F  # å¯¼å…¥PyTorchç¥ç»ç½‘ç»œå‡½æ•°æ¨¡å—
from util.time import *  # ä»util.timeæ¨¡å—å¯¼å…¥æ‰€æœ‰å‡½æ•°
from util.env import *  # ä»util.envæ¨¡å—å¯¼å…¥æ‰€æœ‰å‡½æ•°
from util.debug_logger import get_logger  # ä»util.debug_loggeræ¨¡å—å¯¼å…¥æ—¥å¿—å™¨
from torch_geometric.nn import GCNConv, GATConv, EdgeConv  # ä»PyTorch Geometricå¯¼å…¥å›¾å·ç§¯å±‚

from .graph_layer import GraphLayer  # ä»å½“å‰åŒ…å¯¼å…¥GraphLayerç±»


def get_batch_edge_index(org_edge_index, batch_num, node_num):
    """
    å°†è¾¹ç´¢å¼•æ‰©å±•åˆ°batchç»´åº¦

    Args:
        org_edge_index: åŸå§‹è¾¹ç´¢å¼• [2, edge_num]
        batch_num: batchå¤§å°
        node_num: èŠ‚ç‚¹æ•°é‡

    Returns:
        batch_edge_index: æ‰©å±•åçš„è¾¹ç´¢å¼• [2, edge_num * batch_num]
    """
    edge_index = org_edge_index.clone().detach()  # å¤åˆ¶åŸå§‹è¾¹ç´¢å¼•å¹¶åˆ†ç¦»æ¢¯åº¦
    edge_num = org_edge_index.shape[1]  # è·å–è¾¹æ•°é‡
    batch_edge_index = edge_index.repeat(1, batch_num).contiguous()  # é‡å¤è¾¹ç´¢å¼•ä»¥é€‚åº”æ‰¹æ¬¡å¤§å°

    for i in range(batch_num):  # éå†æ¯ä¸ªæ‰¹æ¬¡
        batch_edge_index[:, i * edge_num:(i + 1) * edge_num] += i * node_num  # ä¸ºæ¯ä¸ªæ‰¹æ¬¡çš„èŠ‚ç‚¹æ·»åŠ åç§»

    return batch_edge_index.long()  # è¿”å›é•¿æ•´å‹çš„æ‰¹æ¬¡è¾¹ç´¢å¼•


class OutLayer(nn.Module):
    """
    è¾“å‡ºå±‚: å¤šå±‚æ„ŸçŸ¥æœº(MLP)
    å°†GNNè¾“å‡ºæ˜ å°„åˆ°æœ€ç»ˆé¢„æµ‹å€¼
    """
    def __init__(self, in_num, node_num, layer_num, inter_num=512):
        # åˆå§‹åŒ–è¾“å‡ºå±‚
        super(OutLayer, self).__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•

        modules = []  # åˆå§‹åŒ–æ¨¡å—åˆ—è¡¨

        for i in range(layer_num):  # éå†æ¯å±‚
            # last layer, output shape:1
            if i == layer_num - 1:  # å¦‚æœæ˜¯æœ€åä¸€å±‚
                # æ·»åŠ çº¿æ€§å±‚ï¼Œè¾“å‡ºç»´åº¦ä¸º1
                modules.append(nn.Linear(in_num if layer_num == 1 else inter_num, 1))
            else:  # å¦‚æœä¸æ˜¯æœ€åä¸€å±‚
                layer_in_num = in_num if i == 0 else inter_num  # ç¡®å®šè¾“å…¥ç»´åº¦
                modules.append(nn.Linear(layer_in_num, inter_num))  # æ·»åŠ çº¿æ€§å±‚
                modules.append(nn.BatchNorm1d(inter_num))  # æ·»åŠ æ‰¹å½’ä¸€åŒ–å±‚
                modules.append(nn.ReLU())  # æ·»åŠ ReLUæ¿€æ´»å‡½æ•°

        self.mlp = nn.ModuleList(modules)  # å°†æ¨¡å—åˆ—è¡¨è½¬æ¢ä¸ºModuleList

    def forward(self, x):
        # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•
        out = x  # åˆå§‹åŒ–è¾“å‡ºä¸ºè¾“å…¥

        for mod in self.mlp:  # éå†MLPä¸­çš„æ¯ä¸ªæ¨¡å—
            if isinstance(mod, nn.BatchNorm1d):  # å¦‚æœæ˜¯æ‰¹å½’ä¸€åŒ–å±‚
                out = out.permute(0, 2, 1)  # è°ƒæ•´ç»´åº¦é¡ºåº
                out = mod(out)  # åº”ç”¨æ‰¹å½’ä¸€åŒ–
                out = out.permute(0, 2, 1)  # æ¢å¤ç»´åº¦é¡ºåº
            else:  # å¦‚æœä¸æ˜¯æ‰¹å½’ä¸€åŒ–å±‚
                out = mod(out)  # åº”ç”¨æ¨¡å—

        return out  # è¿”å›è¾“å‡º


class GNNLayer(nn.Module):
    """
    GNNå±‚: å›¾æ³¨æ„åŠ›å±‚ + BatchNorm + ReLU
    """
    def __init__(self, in_channel, out_channel, inter_dim=0, heads=1, node_num=100):
        # åˆå§‹åŒ–GNNå±‚
        super(GNNLayer, self).__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•

        self.gnn = GraphLayer(
            in_channel, out_channel, inter_dim=inter_dim, heads=heads, concat=False
        )  # åˆ›å»ºGraphLayerå®ä¾‹

        self.bn = nn.BatchNorm1d(out_channel)  # åˆ›å»ºæ‰¹å½’ä¸€åŒ–å±‚
        self.relu = nn.ReLU()  # åˆ›å»ºReLUæ¿€æ´»å‡½æ•°
        self.leaky_relu = nn.LeakyReLU()  # åˆ›å»ºLeakyReLUæ¿€æ´»å‡½æ•°

    def forward(self, x, edge_index, embedding=None, node_num=0):
        """
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [batch*node_num, feature_dim]
            edge_index: è¾¹ç´¢å¼• [2, edge_num]
            embedding: èŠ‚ç‚¹åµŒå…¥
            node_num: èŠ‚ç‚¹æ•°é‡

        Returns:
            out: è¾“å‡ºç‰¹å¾ [batch*node_num, out_channel]
        """
        # æ‰§è¡Œå›¾å±‚å‰å‘ä¼ æ’­
        out, (new_edge_index, att_weight) = self.gnn(
            x, edge_index, embedding, return_attention_weights=True
        )
        self.att_weight_1 = att_weight  # ä¿å­˜æ³¨æ„åŠ›æƒé‡
        self.edge_index_1 = new_edge_index  # ä¿å­˜è¾¹ç´¢å¼•

        out = self.bn(out)  # åº”ç”¨æ‰¹å½’ä¸€åŒ–

        return self.relu(out)  # åº”ç”¨ReLUæ¿€æ´»å‡½æ•°å¹¶è¿”å›è¾“å‡º


class GDN(nn.Module):
    """
    GDN (Graph Deviation Network) ä¸»æ¨¡å‹

    å·¥ä½œæµç¨‹:
    1. èŠ‚ç‚¹åµŒå…¥å­¦ä¹ 
    2. åŸºäºåµŒå…¥çš„å›¾ç»“æ„å­¦ä¹ ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ + TopKï¼‰
    3. GNNæ¶ˆæ¯ä¼ é€’
    4. è¾“å‡ºå±‚é¢„æµ‹
    """
    def __init__(
            self, edge_index_sets, node_num, dim=64, out_layer_inter_dim=256,
            input_dim=10, out_layer_num=1, topk=20
    ):
        # åˆå§‹åŒ–GDNæ¨¡å‹
        super(GDN, self).__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•

        self.edge_index_sets = edge_index_sets  # å­˜å‚¨è¾¹ç´¢å¼•é›†åˆ

        device = get_device()  # è·å–è®¾å¤‡

        edge_index = edge_index_sets[0]  # è·å–ç¬¬ä¸€ä¸ªè¾¹ç´¢å¼•

        embed_dim = dim  # è®¾ç½®åµŒå…¥ç»´åº¦
        self.embedding = nn.Embedding(node_num, embed_dim)  # åˆ›å»ºèŠ‚ç‚¹åµŒå…¥å±‚
        self.bn_outlayer_in = nn.BatchNorm1d(embed_dim)  # åˆ›å»ºè¾“å‡ºå±‚è¾“å…¥æ‰¹å½’ä¸€åŒ–å±‚

        edge_set_num = len(edge_index_sets)  # è·å–è¾¹é›†åˆæ•°é‡
        self.gnn_layers = nn.ModuleList([
            # ä¸ºæ¯ä¸ªè¾¹é›†åˆåˆ›å»ºGNNå±‚
            GNNLayer(input_dim, dim, inter_dim=dim + embed_dim, heads=1)
            for i in range(edge_set_num)
        ])

        self.node_embedding = None  # åˆå§‹åŒ–èŠ‚ç‚¹åµŒå…¥ä¸ºNone
        self.topk = topk  # å­˜å‚¨TopKå‚æ•°
        self.learned_graph = None  # åˆå§‹åŒ–å­¦ä¹ åˆ°çš„å›¾ä¸ºNone

        # åˆ›å»ºè¾“å‡ºå±‚
        self.out_layer = OutLayer(
            dim * edge_set_num, node_num, out_layer_num, inter_num=out_layer_inter_dim
        )

        self.cache_edge_index_sets = [None] * edge_set_num  # åˆå§‹åŒ–è¾¹ç´¢å¼•ç¼“å­˜åˆ—è¡¨
        self.cache_embed_index = None  # åˆå§‹åŒ–åµŒå…¥ç´¢å¼•ç¼“å­˜ä¸ºNone

        self.dp = nn.Dropout(0.2)  # åˆ›å»ºDropoutå±‚

        # ç”¨äºæ§åˆ¶forwardæ—¥å¿—çš„è®¡æ•°å™¨
        self._forward_count = 0  # åˆå§‹åŒ–å‰å‘ä¼ æ’­è®¡æ•°å™¨

        self.init_params()  # åˆå§‹åŒ–å‚æ•°

    def init_params(self):
        """åˆå§‹åŒ–å‚æ•°: ä½¿ç”¨Kaimingåˆå§‹åŒ–èŠ‚ç‚¹åµŒå…¥"""
        # ä½¿ç”¨Kaimingæ–¹æ³•åˆå§‹åŒ–åµŒå…¥æƒé‡
        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))

    def forward(self, data, org_edge_index):
        """
        å‰å‘ä¼ æ’­

        Args:
            data: è¾“å…¥æ•°æ® [batch_size, node_num, feature_dim]
            org_edge_index: åŸå§‹è¾¹ç´¢å¼•

        Returns:
            out: é¢„æµ‹è¾“å‡º [batch_size, node_num]
        """
        # è·å–æ—¥å¿—å™¨
        logger = get_logger()  # è·å–æ—¥å¿—å™¨å®ä¾‹

        # åªåœ¨ç¬¬ä¸€æ¬¡forwardæ—¶æ‰“å°è¯¦ç»†æ—¥å¿—
        should_log = logger.debug and logger.debug_forward and self._forward_count == 0  # åˆ¤æ–­æ˜¯å¦éœ€è¦è®°å½•æ—¥å¿—
        self._forward_count += 1  # å¢åŠ å‰å‘ä¼ æ’­è®¡æ•°

        x = data.clone().detach()  # å¤åˆ¶è¾“å…¥æ•°æ®å¹¶åˆ†ç¦»æ¢¯åº¦
        edge_index_sets = self.edge_index_sets  # è·å–è¾¹ç´¢å¼•é›†åˆ

        device = data.device  # è·å–æ•°æ®è®¾å¤‡

        batch_num, node_num, all_feature = x.shape  # è·å–æ‰¹æ¬¡å¤§å°ã€èŠ‚ç‚¹æ•°é‡å’Œç‰¹å¾ç»´åº¦

        if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
            logger.log_subsection("GDN Forward è¿‡ç¨‹", icon='ğŸ§ ')  # è®°å½•å‰å‘ä¼ æ’­è¿‡ç¨‹æ ‡é¢˜
            # è®°å½•è¾“å…¥æ•°æ®ä¿¡æ¯
            logger.log_forward_step(
                f"1. è¾“å…¥æ•°æ®", x, f"batch={batch_num}, nodes={node_num}, features={all_feature}"
            )

        x = x.view(-1, all_feature).contiguous()  # å°†è¾“å…¥å±•å¹³ä¸ºäºŒç»´å¼ é‡

        if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
            logger.log_forward_step("2. å±•å¹³è¾“å…¥", x, f"åˆå¹¶batchå’Œnodeç»´åº¦")  # è®°å½•å±•å¹³æ“ä½œ

        gcn_outs = []  # åˆå§‹åŒ–GNNè¾“å‡ºåˆ—è¡¨
        for i, edge_index in enumerate(edge_index_sets):  # éå†è¾¹ç´¢å¼•é›†åˆ
            edge_num = edge_index.shape[1]  # è·å–è¾¹æ•°é‡
            cache_edge_index = self.cache_edge_index_sets[i]  # è·å–ç¼“å­˜çš„è¾¹ç´¢å¼•

            # å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–å½¢çŠ¶ä¸åŒ¹é…
            if cache_edge_index is None or cache_edge_index.shape[1] != edge_num * batch_num:
                # ç”Ÿæˆæ‰¹æ¬¡è¾¹ç´¢å¼•å¹¶ç¼“å­˜
                self.cache_edge_index_sets[i] = get_batch_edge_index(
                    edge_index, batch_num, node_num
                ).to(device)

            batch_edge_index = self.cache_edge_index_sets[i]  # è·å–æ‰¹æ¬¡è¾¹ç´¢å¼•

            # è·å–èŠ‚ç‚¹åµŒå…¥
            all_embeddings = self.embedding(torch.arange(node_num).to(device))  # è·å–æ‰€æœ‰èŠ‚ç‚¹åµŒå…¥

            if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
                logger.log_forward_step("3. èŠ‚ç‚¹åµŒå…¥", all_embeddings)  # è®°å½•èŠ‚ç‚¹åµŒå…¥ä¿¡æ¯

            weights_arr = all_embeddings.detach().clone()  # åˆ†ç¦»å¹¶å¤åˆ¶åµŒå…¥æƒé‡
            all_embeddings = all_embeddings.repeat(batch_num, 1)  # ä¸ºæ¯ä¸ªæ‰¹æ¬¡é‡å¤åµŒå…¥

            weights = weights_arr.view(node_num, -1)  # é‡å¡‘æƒé‡çŸ©é˜µ

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
            cos_ji_mat = torch.matmul(weights, weights.T)  # è®¡ç®—æƒé‡çš„çŸ©é˜µä¹˜ç§¯
            # è®¡ç®—æƒé‡çš„èŒƒæ•°ä¹˜ç§¯
            normed_mat = torch.matmul(
                weights.norm(dim=-1).view(-1, 1),
                weights.norm(dim=-1).view(1, -1)
            )
            cos_ji_mat = cos_ji_mat / normed_mat  # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ

            if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
                # è®°å½•ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µä¿¡æ¯
                logger.log_forward_step(
                    "4. ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ", cos_ji_mat,
                    f"min={cos_ji_mat.min().item():.3f}, max={cos_ji_mat.max().item():.3f}"
                )

            dim = weights.shape[-1]  # è·å–æƒé‡ç»´åº¦
            topk_num = self.topk  # è·å–TopKæ•°é‡

            # TopKé€‰æ‹©é‚»å±…
            topk_indices_ji = torch.topk(cos_ji_mat, topk_num, dim=-1)[1]  # é€‰æ‹©TopKæœ€ç›¸ä¼¼çš„èŠ‚ç‚¹

            if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
                # è®°å½•TopKé‚»å±…é€‰æ‹©ç»“æœ
                logger.log_forward_step(
                    f"5. TopKé‚»å±…é€‰æ‹©", topk_indices_ji, f"æ¯èŠ‚ç‚¹{topk_num}ä¸ªé‚»å±…"
                )

            self.learned_graph = topk_indices_ji  # ä¿å­˜å­¦ä¹ åˆ°çš„å›¾ç»“æ„

            # åˆ›å»ºç›®æ ‡èŠ‚ç‚¹ç´¢å¼•
            gated_i = torch.arange(0, node_num).unsqueeze(1).repeat(1, topk_num).flatten().to(
                device
            ).unsqueeze(0)
            gated_j = topk_indices_ji.flatten().unsqueeze(0)  # åˆ›å»ºæºèŠ‚ç‚¹ç´¢å¼•
            gated_edge_index = torch.cat((gated_j, gated_i), dim=0)  # æ„å»ºé—¨æ§è¾¹ç´¢å¼•

            if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
                # è®°å½•å­¦ä¹ åˆ°çš„è¾¹ä¿¡æ¯
                logger.log_forward_step(
                    "6. å­¦ä¹ åˆ°çš„è¾¹", gated_edge_index, f"æ€»è¾¹æ•°={gated_edge_index.shape[1]}"
                )

            # è·å–æ‰¹æ¬¡é—¨æ§è¾¹ç´¢å¼•
            batch_gated_edge_index = get_batch_edge_index(
                gated_edge_index, batch_num, node_num
            ).to(device)
            # æ‰§è¡ŒGNNå±‚å‰å‘ä¼ æ’­
            gcn_out = self.gnn_layers[i](x, batch_gated_edge_index, embedding=all_embeddings)

            if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
                logger.log_forward_step(f"7. GNNå±‚{i}è¾“å‡º", gcn_out)  # è®°å½•GNNå±‚è¾“å‡ºä¿¡æ¯

            gcn_outs.append(gcn_out)  # å°†GNNè¾“å‡ºæ·»åŠ åˆ°åˆ—è¡¨

        x = torch.cat(gcn_outs, dim=1)  # æ‹¼æ¥æ‰€æœ‰GNNè¾“å‡º

        if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
            logger.log_forward_step("8. æ‹¼æ¥GNNè¾“å‡º", x)  # è®°å½•æ‹¼æ¥åçš„GNNè¾“å‡º

        x = x.view(batch_num, node_num, -1)  # å°†è¾“å‡ºé‡å¡‘ä¸ºä¸‰ç»´å¼ é‡

        indexes = torch.arange(0, node_num).to(device)  # åˆ›å»ºèŠ‚ç‚¹ç´¢å¼•
        out = torch.mul(x, self.embedding(indexes))  # å°†è¾“å‡ºä¸èŠ‚ç‚¹åµŒå…¥ç›¸ä¹˜

        if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
            logger.log_forward_step("9. åµŒå…¥ä¹˜ç§¯", out)  # è®°å½•åµŒå…¥ä¹˜ç§¯ç»“æœ

        out = out.permute(0, 2, 1)  # è°ƒæ•´ç»´åº¦é¡ºåº
        out = F.relu(self.bn_outlayer_in(out))  # åº”ç”¨æ‰¹å½’ä¸€åŒ–å’ŒReLUæ¿€æ´»å‡½æ•°
        out = out.permute(0, 2, 1)  # æ¢å¤ç»´åº¦é¡ºåº

        if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
            logger.log_forward_step("10. BatchNorm + ReLU", out)  # è®°å½•æ‰¹å½’ä¸€åŒ–å’ŒReLUç»“æœ

        out = self.dp(out)  # åº”ç”¨Dropout
        out = self.out_layer(out)  # é€šè¿‡è¾“å‡ºå±‚
        out = out.view(-1, node_num)  # å°†è¾“å‡ºé‡å¡‘ä¸ºäºŒç»´å¼ é‡

        if should_log:  # å¦‚æœéœ€è¦è®°å½•æ—¥å¿—
            logger.log_forward_step("11. æœ€ç»ˆè¾“å‡º", out)  # è®°å½•æœ€ç»ˆè¾“å‡º

        return out  # è¿”å›æœ€ç»ˆè¾“å‡º
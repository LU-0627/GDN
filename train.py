import numpy as np
import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import time
from util.time import *
from util.env import *
from util.debug_logger import get_logger
from sklearn.metrics import mean_squared_error
from test import *
import torch.nn.functional as F
import numpy as np
from evaluate import get_best_performance_data, get_val_performance_data, get_full_err_scores
from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score
from torch.utils.data import DataLoader, random_split, Subset
from scipy.stats import iqr




def loss_func(y_pred, y_true):
    """
    è®¡ç®—MSEæŸå¤±
    
    Args:
        y_pred: é¢„æµ‹å€¼ [batch_size, node_num]
        y_true: çœŸå®å€¼ [batch_size, node_num]
    
    Returns:
        loss: å‡æ–¹è¯¯å·®æŸå¤±
    """
    loss = F.mse_loss(y_pred, y_true, reduction='mean')

    return loss



def train(model = None, save_path = '', config={},  train_dataloader=None, val_dataloader=None, feature_map={}, test_dataloader=None, test_dataset=None, dataset_name='swat', train_dataset=None):
    """
    è®­ç»ƒGDNæ¨¡å‹
    
    Args:
        model: GDNæ¨¡å‹
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        config: è®­ç»ƒé…ç½®
        train_dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
        feature_map: ç‰¹å¾æ˜ å°„
        test_dataloader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        test_dataset: æµ‹è¯•æ•°æ®é›†
        dataset_name: æ•°æ®é›†åç§°
        train_dataset: è®­ç»ƒæ•°æ®é›†
    
    Returns:
        train_loss_list: è®­ç»ƒæŸå¤±åˆ—è¡¨
    """
    # è·å–æ—¥å¿—å™¨
    logger = get_logger()
    
    logger.log_section("è®­ç»ƒé˜¶æ®µ", icon='ğŸ¯')

    seed = config['seed']

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=config['decay'])
    
    logger.log_subsection("ä¼˜åŒ–å™¨é…ç½®", icon='âš™ï¸')
    logger.log("ä¼˜åŒ–å™¨", "Adam")
    logger.log("å­¦ä¹ ç‡", 0.001)
    logger.log("æƒé‡è¡°å‡", config['decay'])

    now = time.time()
    
    train_loss_list = []
    cmp_loss_list = []

    device = get_device()


    acu_loss = 0
    min_loss = 1e+8
    min_f1 = 0
    min_pre = 0
    best_prec = 0

    i = 0
    epoch = config['epoch']
    early_stop_win = 15
    
    logger.log("æ€»Epochæ•°", epoch)
    logger.log("æ—©åœçª—å£", early_stop_win)

    model.train()

    log_interval = 1000
    stop_improve_count = 0

    dataloader = train_dataloader
    total_batches = len(dataloader)
    
    logger.log("æ¯Epochæ‰¹æ¬¡æ•°", total_batches)

    for i_epoch in range(epoch):
        epoch_start_time = time.time()
        
        # æ‰“å°Epochå¼€å§‹
        logger.log_epoch_start(i_epoch, epoch)

        acu_loss = 0
        model.train()
        
        batch_idx = 0
        for x, labels, attack_labels, edge_index in dataloader:
            _start = time.time()

            x, labels, edge_index = [item.float().to(device) for item in [x, labels, edge_index]]
            
            # æ‰“å°Batchä¿¡æ¯
            logger.log_batch(batch_idx, total_batches, {
                'è¾“å…¥x': x,
                'æ ‡ç­¾y': labels,
                'è¾¹ç´¢å¼•': edge_index
            })

            optimizer.zero_grad()
            out = model(x, edge_index).float().to(device)
            loss = loss_func(out, labels)
            
            # æ‰“å°æŸå¤±ä¿¡æ¯ (ä»…åœ¨debug_forwardæ¨¡å¼æˆ–é¦–ä¸ªbatch)
            if batch_idx == 0 or (logger.debug and logger.debug_forward):
                pred_range = (out.min().item(), out.max().item())
                gt_range = (labels.min().item(), labels.max().item())
                logger.log_loss(loss.item(), pred_range, gt_range)
            
            loss.backward()
            
            # æ‰“å°æ¢¯åº¦ä¿¡æ¯
            if batch_idx == 0:
                logger.log_gradient(model)
            
            optimizer.step()

            
            train_loss_list.append(loss.item())
            acu_loss += loss.item()
                
            i += 1
            batch_idx += 1


        # each epoch
        epoch_time = time.time() - epoch_start_time
        avg_loss = acu_loss/len(dataloader)
        
        print('epoch ({} / {}) (Loss:{:.8f}, ACU_loss:{:.8f})'.format(
                        i_epoch, epoch, 
                        avg_loss, acu_loss), flush=True
            )

        # use val dataset to judge
        if val_dataloader is not None:

            val_loss, val_result = test(model, val_dataloader)
            
            is_best = val_loss < min_loss

            if is_best:
                torch.save(model.state_dict(), save_path)

                min_loss = val_loss
                stop_improve_count = 0
            else:
                stop_improve_count += 1
            
            # æ‰“å°Epochç»“æŸä¿¡æ¯
            logger.log_epoch_end(i_epoch, avg_loss, val_loss, best=is_best, time_elapsed=epoch_time)
            
            if stop_improve_count > 0:
                logger.log(f"âš ï¸ æ—©åœè®¡æ•°å™¨", f"{stop_improve_count}/{early_stop_win}")


            if stop_improve_count >= early_stop_win:
                logger.log("âš ï¸ è§¦å‘æ—©åœ", f"è¿ç»­{early_stop_win}ä¸ªepochæ— æ”¹å–„")
                break

        else:
            if acu_loss < min_loss :
                torch.save(model.state_dict(), save_path)
                min_loss = acu_loss
            
            logger.log_epoch_end(i_epoch, avg_loss, time_elapsed=epoch_time)

    # æ‰“å°è®­ç»ƒå®Œæˆä¿¡æ¯
    logger.log_training_complete(i_epoch + 1, min_loss, save_path)

    return train_loss_list

# -*- coding: utf-8 -*-
import pandas as pd  # å¯¼å…¥pandasåº“ï¼Œç”¨äºæ•°æ®å¤„ç†å’Œåˆ†æ
import numpy as np   # å¯¼å…¥numpyåº“ï¼Œç”¨äºæ•°å€¼è®¡ç®—
import torch         # å¯¼å…¥PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import matplotlib.pyplot as plt  # å¯¼å…¥matplotlibç»˜å›¾åº“
from torch.utils.data import DataLoader, random_split, Subset  # ä»PyTorchå¯¼å…¥æ•°æ®åŠ è½½å™¨ç›¸å…³åŠŸèƒ½

from sklearn.preprocessing import MinMaxScaler  # ä»sklearnå¯¼å…¥æ•°æ®ç¼©æ”¾å™¨

from util.env import get_device, set_device  # ä»util.envæ¨¡å—å¯¼å…¥è®¾å¤‡è·å–å’Œè®¾ç½®å‡½æ•°
from util.preprocess import build_loc_net, construct_data  # ä»util.preprocessæ¨¡å—å¯¼å…¥ç½‘ç»œæ„å»ºå’Œæ•°æ®æ„é€ å‡½æ•°
from util.net_struct import get_feature_map, get_fc_graph_struc  # ä»util.net_structæ¨¡å—å¯¼å…¥ç‰¹å¾æ˜ å°„å’Œå…¨è¿æ¥å›¾ç»“æ„è·å–å‡½æ•°
from util.iostream import printsep  # ä»util.iostreamæ¨¡å—å¯¼å…¥æ‰“å°åˆ†éš”ç¬¦å‡½æ•°
from util.debug_logger import DebugLogger, init_global_logger, get_logger  # ä»util.debug_loggeræ¨¡å—å¯¼å…¥è°ƒè¯•æ—¥å¿—ç›¸å…³ç±»å’Œå‡½æ•°

from datasets.TimeDataset import TimeDataset  # ä»datasets.TimeDatasetæ¨¡å—å¯¼å…¥æ—¶é—´åºåˆ—æ•°æ®é›†ç±»

from models.GDN import GDN  # ä»models.GDNæ¨¡å—å¯¼å…¥GDNå›¾ç¥ç»ç½‘ç»œæ¨¡å‹

from train import train  # ä»trainæ¨¡å—å¯¼å…¥è®­ç»ƒå‡½æ•°
from test import test   # ä»testæ¨¡å—å¯¼å…¥æµ‹è¯•å‡½æ•°
from evaluate import get_err_scores, get_best_performance_data, get_val_performance_data, get_full_err_scores  # ä»evaluateæ¨¡å—å¯¼å…¥è¯„ä¼°ç›¸å…³å‡½æ•°

import sys             # å¯¼å…¥ç³»ç»Ÿç›¸å…³åŠŸèƒ½æ¨¡å—
from datetime import datetime  # ä»datetimeæ¨¡å—å¯¼å…¥æ—¥æœŸæ—¶é—´ç±»

import os              # å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£æ¨¡å—
import argparse        # å¯¼å…¥å‘½ä»¤è¡Œå‚æ•°è§£ææ¨¡å—
from pathlib import Path  # ä»pathlibæ¨¡å—å¯¼å…¥è·¯å¾„å¤„ç†ç±»

import json            # å¯¼å…¥JSONå¤„ç†æ¨¡å—
import random          # å¯¼å…¥éšæœºæ•°ç”Ÿæˆæ¨¡å—


class Main():  # å®šä¹‰ä¸»ç±»ï¼Œå°è£…æ•´ä¸ªç¨‹åºçš„ä¸»è¦åŠŸèƒ½
    def __init__(self, train_config, env_config, debug=False, debug_config=None):  # åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥æ”¶è®­ç»ƒé…ç½®ã€ç¯å¢ƒé…ç½®å’Œè°ƒè¯•å‚æ•°
        """
        åˆå§‹åŒ–ä¸»ç¨‹åº
        
        Args:
            train_config: è®­ç»ƒé…ç½®
            env_config: ç¯å¢ƒé…ç½®  
            debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
            debug_config: è°ƒè¯•é…ç½® {'debug_batch': N, 'debug_forward': bool}
        """
        self.train_config = train_config  # å­˜å‚¨è®­ç»ƒé…ç½®
        self.env_config = env_config      # å­˜å‚¨ç¯å¢ƒé…ç½®
        self.datestr = None               # åˆå§‹åŒ–æ—¥æœŸå­—ç¬¦ä¸²å˜é‡
        
        # åˆå§‹åŒ–æ—¥å¿—å™¨
        if debug_config is None:          # å¦‚æœè°ƒè¯•é…ç½®ä¸ºç©º
            debug_config = {}             # åˆ›å»ºç©ºå­—å…¸
        self.logger = init_global_logger(  # åˆå§‹åŒ–å…¨å±€æ—¥å¿—å™¨
            debug=debug,                  # è°ƒè¯•æ¨¡å¼å¼€å…³
            log_dir='./logs',             # æ—¥å¿—ä¿å­˜ç›®å½•
            dataset_name=env_config['dataset'],  # æ•°æ®é›†åç§°
            debug_batch=debug_config.get('debug_batch', 1),  # è°ƒè¯•æ‰¹æ¬¡é—´éš”
            debug_forward=debug_config.get('debug_forward', False)  # æ˜¯å¦è°ƒè¯•å‰å‘ä¼ æ’­
        )
        
        # ========== æ•°æ®åŠ è½½é˜¶æ®µ ==========
        self.logger.log_section("æ•°æ®åŠ è½½", icon='ğŸ“Š')  # è®°å½•æ•°æ®åŠ è½½é˜¶æ®µå¼€å§‹
        
        dataset = self.env_config['dataset']  # è·å–æ•°æ®é›†åç§°
        self.logger.log("æ•°æ®é›†", dataset)      # è®°å½•æ•°æ®é›†åç§°
        
        train_orig = pd.read_csv(f'./data/{dataset}/train.csv', sep=',', index_col=0)  # è¯»å–è®­ç»ƒæ•°æ®
        test_orig = pd.read_csv(f'./data/{dataset}/test.csv', sep=',', index_col=0)    # è¯»å–æµ‹è¯•æ•°æ®
        
        self.logger.log("åŸå§‹è®­ç»ƒæ•°æ®", f"{train_orig.shape[0]} è¡Œ Ã— {train_orig.shape[1]} åˆ—")  # è®°å½•è®­ç»ƒæ•°æ®å½¢çŠ¶
        self.logger.log("åŸå§‹æµ‹è¯•æ•°æ®", f"{test_orig.shape[0]} è¡Œ Ã— {test_orig.shape[1]} åˆ—")    # è®°å½•æµ‹è¯•æ•°æ®å½¢çŠ¶
       
        train, test = train_orig, test_orig  # å°†åŸå§‹æ•°æ®èµ‹å€¼ç»™trainå’Œtestå˜é‡

        if 'attack' in train.columns:        # å¦‚æœè®­ç»ƒæ•°æ®ä¸­æœ‰attackåˆ—
            train = train.drop(columns=['attack'])  # åˆ é™¤attackåˆ—
            self.logger.log("ç§»é™¤è®­ç»ƒé›†attackåˆ—", "æ˜¯")  # è®°å½•åˆ é™¤æ“ä½œ

        feature_map = get_feature_map(dataset)      # è·å–ç‰¹å¾æ˜ å°„
        fc_struc = get_fc_graph_struc(dataset)      # è·å–å…¨è¿æ¥å›¾ç»“æ„
        
        self.logger.log("ç‰¹å¾æ•°é‡", f"{len(feature_map)} ä¸ªä¼ æ„Ÿå™¨")  # è®°å½•ç‰¹å¾æ•°é‡
        self.logger.log("ç‰¹å¾åˆ—è¡¨", str(feature_map[:5]) + "..." if len(feature_map) > 5 else str(feature_map))  # è®°å½•ç‰¹å¾åˆ—è¡¨

        set_device(env_config['device'])  # è®¾ç½®è®¡ç®—è®¾å¤‡
        self.device = get_device()      # è·å–å½“å‰è®¾å¤‡
        self.logger.log("è®¡ç®—è®¾å¤‡", str(self.device))  # è®°å½•è®¡ç®—è®¾å¤‡

        fc_edge_index = build_loc_net(fc_struc, list(train.columns), feature_map=feature_map)  # æ„å»ºå±€éƒ¨ç½‘ç»œè¿æ¥
        fc_edge_index = torch.tensor(fc_edge_index, dtype=torch.long)  # å°†è¾¹ç´¢å¼•è½¬æ¢ä¸ºPyTorchå¼ é‡
        
        self.logger.log_tensor("å›¾ç»“æ„è¾¹ç´¢å¼• (fc_edge_index)", fc_edge_index, show_stats=False)  # è®°å½•å›¾ç»“æ„è¾¹ç´¢å¼•
        self.logger.log("è¾¹æ•°é‡", fc_edge_index.shape[1])  # è®°å½•è¾¹æ•°é‡

        self.feature_map = feature_map  # ä¿å­˜ç‰¹å¾æ˜ å°„

        train_dataset_indata = construct_data(train, feature_map, labels=0)  # æ„é€ è®­ç»ƒæ•°æ®
        test_dataset_indata = construct_data(test, feature_map, labels=test.attack.tolist())  # æ„é€ æµ‹è¯•æ•°æ®
        
        # è®¡ç®—å¼‚å¸¸æ ·æœ¬æ¯”ä¾‹
        attack_labels = test.attack.tolist()  # è·å–æ”»å‡»æ ‡ç­¾åˆ—è¡¨
        n_anomaly = sum(attack_labels)        # è®¡ç®—å¼‚å¸¸æ ·æœ¬æ•°é‡
        n_total = len(attack_labels)          # è®¡ç®—æ€»æ ·æœ¬æ•°é‡
        self.logger.log("æµ‹è¯•é›†å¼‚å¸¸æ ·æœ¬", f"{n_anomaly}/{n_total} ({100*n_anomaly/n_total:.1f}%)")  # è®°å½•å¼‚å¸¸æ ·æœ¬æ¯”ä¾‹

        # ========== æ»‘çª—å¤„ç† ==========
        self.logger.log_subsection("æ»‘çª—å¤„ç†", icon='ğŸ”„')  # è®°å½•æ»‘çª—å¤„ç†å¼€å§‹
        
        cfg = {  # å®šä¹‰æ»‘çª—é…ç½®
            'slide_win': train_config['slide_win'],    # æ»‘åŠ¨çª—å£å¤§å°
            'slide_stride': train_config['slide_stride'],  # æ»‘åŠ¨æ­¥é•¿
        }
        self.logger.log("æ»‘åŠ¨çª—å£", f"{cfg['slide_win']} æ—¶é—´æ­¥")  # è®°å½•æ»‘åŠ¨çª—å£å¤§å°
        self.logger.log("æ»‘åŠ¨æ­¥é•¿", cfg['slide_stride'])  # è®°å½•æ»‘åŠ¨æ­¥é•¿

        train_dataset = TimeDataset(train_dataset_indata, fc_edge_index, mode='train', config=cfg)  # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        test_dataset = TimeDataset(test_dataset_indata, fc_edge_index, mode='test', config=cfg)    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        
        self.logger.log("è®­ç»ƒé›†æ ·æœ¬æ•°", len(train_dataset))  # è®°å½•è®­ç»ƒé›†æ ·æœ¬æ•°é‡
        self.logger.log("æµ‹è¯•é›†æ ·æœ¬æ•°", len(test_dataset))    # è®°å½•æµ‹è¯•é›†æ ·æœ¬æ•°é‡
        
        # æ‰“å°å•ä¸ªæ ·æœ¬çš„shape
        sample_x, sample_y, sample_label, sample_edge = train_dataset[0]  # è·å–è®­ç»ƒé›†ç¬¬ä¸€ä¸ªæ ·æœ¬
        self.logger.log_tensor("å•æ ·æœ¬x (å†å²çª—å£)", sample_x, show_stats=False)  # è®°å½•è¾“å…¥å¼ é‡ä¿¡æ¯
        self.logger.log_tensor("å•æ ·æœ¬y (é¢„æµ‹ç›®æ ‡)", sample_y, show_stats=False)  # è®°å½•è¾“å‡ºå¼ é‡ä¿¡æ¯

        train_dataloader, val_dataloader = self.get_loaders(
            train_dataset, 
            train_config['seed'], 
            train_config['batch'], 
            val_ratio=train_config['val_ratio']
        )  # è·å–è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨

        self.train_dataset = train_dataset  # ä¿å­˜è®­ç»ƒæ•°æ®é›†
        self.test_dataset = test_dataset    # ä¿å­˜æµ‹è¯•æ•°æ®é›†

        self.train_dataloader = train_dataloader  # ä¿å­˜è®­ç»ƒæ•°æ®åŠ è½½å™¨
        self.val_dataloader = val_dataloader      # ä¿å­˜éªŒè¯æ•°æ®åŠ è½½å™¨
        self.test_dataloader = DataLoader(
            test_dataset, 
            batch_size=train_config['batch'],
            shuffle=False, 
            num_workers=0
        )  # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
        
        self.logger.log_subsection("æ•°æ®åŠ è½½å™¨", icon='ğŸ“¦')  # è®°å½•æ•°æ®åŠ è½½å™¨ä¿¡æ¯
        self.logger.log("æ‰¹æ¬¡å¤§å°", train_config['batch'])  # è®°å½•æ‰¹æ¬¡å¤§å°
        self.logger.log("è®­ç»ƒæ‰¹æ¬¡æ•°", len(train_dataloader))  # è®°å½•è®­ç»ƒæ‰¹æ¬¡æ•°
        self.logger.log("éªŒè¯æ‰¹æ¬¡æ•°", len(val_dataloader))    # è®°å½•éªŒè¯æ‰¹æ¬¡æ•°
        self.logger.log("æµ‹è¯•æ‰¹æ¬¡æ•°", len(self.test_dataloader))  # è®°å½•æµ‹è¯•æ‰¹æ¬¡æ•°

        # ========== æ¨¡å‹åˆå§‹åŒ– ==========
        self.logger.log_section("æ¨¡å‹åˆå§‹åŒ–", icon='ğŸ—ï¸')  # è®°å½•æ¨¡å‹åˆå§‹åŒ–å¼€å§‹
        
        edge_index_sets = []      # åˆå§‹åŒ–è¾¹ç´¢å¼•é›†åˆ
        edge_index_sets.append(fc_edge_index)  # æ·»åŠ å…¨è¿æ¥è¾¹ç´¢å¼•

        self.model = GDN(
            edge_index_sets, 
            len(feature_map),
            dim=train_config['dim'], 
            input_dim=train_config['slide_win'],
            out_layer_num=train_config['out_layer_num'],
            out_layer_inter_dim=train_config['out_layer_inter_dim'],
            topk=train_config['topk']
        ).to(self.device)  # åˆ›å»ºGDNæ¨¡å‹å®ä¾‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        
        self.logger.log("èŠ‚ç‚¹æ•°é‡", len(feature_map))  # è®°å½•èŠ‚ç‚¹æ•°é‡
        self.logger.log("åµŒå…¥ç»´åº¦", train_config['dim'])  # è®°å½•åµŒå…¥ç»´åº¦
        self.logger.log("è¾“å…¥çª—å£é•¿åº¦", train_config['slide_win'])  # è®°å½•è¾“å…¥çª—å£é•¿åº¦
        self.logger.log("Top-Ké‚»å±…æ•°", train_config['topk'])  # è®°å½•Top-Ké‚»å±…æ•°
        self.logger.log("è¾“å‡ºå±‚æ•°", train_config['out_layer_num'])  # è®°å½•è¾“å‡ºå±‚æ•°
        self.logger.log("è¾“å‡ºå±‚ä¸­é—´ç»´åº¦", train_config['out_layer_inter_dim'])  # è®°å½•è¾“å‡ºå±‚ä¸­é—´ç»´åº¦
        self.logger.log_model_summary(self.model)  # è®°å½•æ¨¡å‹æ‘˜è¦ä¿¡æ¯

    def run(self):  # å®šä¹‰è¿è¡Œæ–¹æ³•
        if len(self.env_config['load_model_path']) > 0:  # å¦‚æœæœ‰é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
            model_save_path = self.env_config['load_model_path']  # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        else:  # å¦åˆ™
            model_save_path = self.get_save_path()[0]  # è·å–æ–°çš„ä¿å­˜è·¯å¾„

            self.train_log = train(
                self.model, 
                model_save_path,
                config=train_config,
                train_dataloader=self.train_dataloader,
                val_dataloader=self.val_dataloader, 
                feature_map=self.feature_map,
                test_dataloader=self.test_dataloader,
                test_dataset=self.test_dataset,
                train_dataset=self.train_dataset,
                dataset_name=self.env_config['dataset']
            )  # å¼€å§‹è®­ç»ƒæ¨¡å‹
        
        # test            
        self.model.load_state_dict(torch.load(model_save_path))  # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡
        best_model = self.model.to(self.device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡

        _, self.test_result = test(best_model, self.test_dataloader)  # å¯¹æµ‹è¯•é›†è¿›è¡Œæµ‹è¯•
        _, self.val_result = test(best_model, self.val_dataloader)    # å¯¹éªŒè¯é›†è¿›è¡Œæµ‹è¯•

        self.get_score(self.test_result, self.val_result)  # è®¡ç®—è¯„ä¼°åˆ†æ•°

    def get_loaders(self, train_dataset, seed, batch, val_ratio=0.1):  # å®šä¹‰è·å–æ•°æ®åŠ è½½å™¨çš„æ–¹æ³•
        dataset_len = int(len(train_dataset))  # è·å–æ•°æ®é›†é•¿åº¦
        train_use_len = int(dataset_len * (1 - val_ratio))  # è®¡ç®—è®­ç»ƒé›†ä½¿ç”¨é•¿åº¦
        val_use_len = int(dataset_len * val_ratio)          # è®¡ç®—éªŒè¯é›†ä½¿ç”¨é•¿åº¦
        val_start_index = random.randrange(train_use_len)   # éšæœºé€‰æ‹©éªŒè¯é›†èµ·å§‹ç´¢å¼•
        indices = torch.arange(dataset_len)                 # åˆ›å»ºç´¢å¼•å¼ é‡

        train_sub_indices = torch.cat([indices[:val_start_index], indices[val_start_index+val_use_len:]])  # æ„å»ºè®­ç»ƒå­ç´¢å¼•
        train_subset = Subset(train_dataset, train_sub_indices)  # åˆ›å»ºè®­ç»ƒå­é›†

        val_sub_indices = indices[val_start_index:val_start_index+val_use_len]  # æ„å»ºéªŒè¯å­ç´¢å¼•
        val_subset = Subset(train_dataset, val_sub_indices)  # åˆ›å»ºéªŒè¯å­é›†

        train_dataloader = DataLoader(
            train_subset, 
            batch_size=batch,
            shuffle=True
        )  # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨

        val_dataloader = DataLoader(
            val_subset, 
            batch_size=batch,
            shuffle=False
        )  # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨

        return train_dataloader, val_dataloader  # è¿”å›è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨

    def get_score(self, test_result, val_result):  # å®šä¹‰è·å–è¯„åˆ†çš„æ–¹æ³•
        """
        è®¡ç®—å¹¶æ‰“å°è¯„ä¼°åˆ†æ•°
        
        Args:
            test_result: æµ‹è¯•ç»“æœ
            val_result: éªŒè¯ç»“æœ
        """
        feature_num = len(test_result[0][0])  # è·å–ç‰¹å¾æ•°é‡
        np_test_result = np.array(test_result)  # å°†æµ‹è¯•ç»“æœè½¬æ¢ä¸ºnumpyæ•°ç»„
        np_val_result = np.array(val_result)    # å°†éªŒè¯ç»“æœè½¬æ¢ä¸ºnumpyæ•°ç»„

        test_labels = np_test_result[2, :, 0].tolist()  # è·å–æµ‹è¯•æ ‡ç­¾
    
        test_scores, normal_scores = get_full_err_scores(test_result, val_result)  # è·å–å®Œæ•´é”™è¯¯è¯„åˆ†

        top1_best_info = get_best_performance_data(test_scores, test_labels, topk=1)   # è·å–æœ€ä½³æ€§èƒ½æ•°æ®
        top1_val_info = get_val_performance_data(test_scores, normal_scores, test_labels, topk=1)  # è·å–éªŒè¯æ€§èƒ½æ•°æ®

        # ä½¿ç”¨æ—¥å¿—å™¨æ‰“å°æœ€ç»ˆç»“æœ
        self.logger.log_section("æœ€ç»ˆè¯„ä¼°ç»“æœ", icon='ğŸ†')  # è®°å½•è¯„ä¼°ç»“æœéƒ¨åˆ†å¼€å§‹
        
        # æ‰“å°ä¸¤ç§è¯„ä¼°æ–¹å¼çš„å¯¹æ¯”
        self.logger.log_subsection("æœ€ä¼˜é˜ˆå€¼ç»“æœ (Best)", icon='â­')  # è®°å½•æœ€ä¼˜é˜ˆå€¼ç»“æœ
        self.logger.log_evaluation_result({
            'F1 Score': top1_best_info[0],
            'Precision': top1_best_info[1],
            'Recall': top1_best_info[2],
            'ROC-AUC': top1_best_info[3],
            'Threshold': top1_best_info[4],
        })
        
        self.logger.log_subsection("éªŒè¯é˜ˆå€¼ç»“æœ (Val)", icon='ğŸ“‹')  # è®°å½•éªŒè¯é˜ˆå€¼ç»“æœ
        self.logger.log_evaluation_result({
            'F1 Score': top1_val_info[0],
            'Precision': top1_val_info[1],
            'Recall': top1_val_info[2],
            'ROC-AUC': top1_val_info[3],
            'Threshold': top1_val_info[4],
        })

        print('=========================** Result **============================\n')  # æ‰“å°ç»“æœåˆ†éš”ç¬¦

        info = None
        if self.env_config['report'] == 'best':  # å¦‚æœæŠ¥å‘Šæ¨¡å¼ä¸ºbest
            info = top1_best_info  # ä½¿ç”¨æœ€ä½³æ€§èƒ½ä¿¡æ¯
        elif self.env_config['report'] == 'val':  # å¦‚æœæŠ¥å‘Šæ¨¡å¼ä¸ºval
            info = top1_val_info   # ä½¿ç”¨éªŒè¯æ€§èƒ½ä¿¡æ¯

        print(f'F1 score: {info[0]}')      # æ‰“å°F1åˆ†æ•°
        print(f'precision: {info[1]}')     # æ‰“å°ç²¾ç¡®ç‡
        print(f'recall: {info[2]}\n')      # æ‰“å°å¬å›ç‡

    def get_save_path(self, feature_name=''):  # å®šä¹‰è·å–ä¿å­˜è·¯å¾„çš„æ–¹æ³•
        dir_path = self.env_config['save_path']  # è·å–ä¿å­˜è·¯å¾„
        
        if self.datestr is None:      # å¦‚æœæ—¥æœŸå­—ç¬¦ä¸²ä¸ºç©º
            now = datetime.now()      # è·å–å½“å‰æ—¶é—´
            self.datestr = now.strftime('%m|%d-%H-%M-%S')  # æ ¼å¼åŒ–æ—¶é—´ä¸ºå­—ç¬¦ä¸²(Windowså…¼å®¹)
        datestr = self.datestr          

        paths = [  # å®šä¹‰è·¯å¾„åˆ—è¡¨
            f'./pretrained/{dir_path}/best_{datestr}.pt',  # æ¨¡å‹ä¿å­˜è·¯å¾„
            f'./results/{dir_path}/{datestr}.csv',         # ç»“æœä¿å­˜è·¯å¾„
        ]

        for path in paths:              # éå†è·¯å¾„åˆ—è¡¨
            dirname = os.path.dirname(path)  # è·å–ç›®å½•å
            Path(dirname).mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•

        return paths  # è¿”å›è·¯å¾„åˆ—è¡¨


if __name__ == "__main__":  # å½“è„šæœ¬ä½œä¸ºä¸»ç¨‹åºè¿è¡Œæ—¶
    parser = argparse.ArgumentParser()  # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨

    parser.add_argument('-batch', help='batch size', type=int, default=128)  # æ‰¹æ¬¡å¤§å°å‚æ•°
    parser.add_argument('-epoch', help='train epoch', type=int, default=100)  # è®­ç»ƒè½®æ¬¡å‚æ•°
    parser.add_argument('-slide_win', help='slide_win', type=int, default=15)  # æ»‘åŠ¨çª—å£å‚æ•°
    parser.add_argument('-dim', help='dimension', type=int, default=64)       # ç»´åº¦å‚æ•°
    parser.add_argument('-slide_stride', help='slide_stride', type=int, default=5)  # æ»‘åŠ¨æ­¥é•¿å‚æ•°
    parser.add_argument('-save_path_pattern', help='save path pattern', type=str, default='')  # ä¿å­˜è·¯å¾„æ¨¡å¼å‚æ•°
    parser.add_argument('-dataset', help='wadi / swat', type=str, default='wadi')  # æ•°æ®é›†å‚æ•°
    parser.add_argument('-device', help='cuda / cpu', type=str, default='cuda')   # è®¾å¤‡å‚æ•°
    parser.add_argument('-random_seed', help='random seed', type=int, default=0)  # éšæœºç§å­å‚æ•°
    parser.add_argument('-comment', help='experiment comment', type=str, default='')  # å®éªŒæ³¨é‡Šå‚æ•°
    parser.add_argument('-out_layer_num', help='outlayer num', type=int, default=1)  # è¾“å‡ºå±‚æ•°å‚æ•°
    parser.add_argument('-out_layer_inter_dim', help='out_layer_inter_dim', type=int, default=256)  # è¾“å‡ºå±‚ä¸­é—´ç»´åº¦å‚æ•°
    parser.add_argument('-decay', help='decay', type=float, default=0)  # è¡°å‡å‚æ•°
    parser.add_argument('-val_ratio', help='val ratio', type=float, default=0.1)  # éªŒè¯æ¯”ä¾‹å‚æ•°
    parser.add_argument('-topk', help='topk num', type=int, default=20)  # topkå‚æ•°
    parser.add_argument('-report', help='best / val', type=str, default='best')  # æŠ¥å‘Šæ¨¡å¼å‚æ•°
    parser.add_argument('-load_model_path', help='trained model path', type=str, default='')  # åŠ è½½æ¨¡å‹è·¯å¾„å‚æ•°
    
    # è°ƒè¯•å‚æ•°
    parser.add_argument('--debug', help='å¼€å¯è°ƒè¯•æ—¥å¿—', action='store_true')  # è°ƒè¯•æ¨¡å¼å‚æ•°
    parser.add_argument('--debug_batch', help='æ¯Nä¸ªbatchæ‰“å°ä¸€æ¬¡', type=int, default=1)  # è°ƒè¯•æ‰¹æ¬¡å‚æ•°
    parser.add_argument('--debug_forward', help='æ‰“å°forwardå†…éƒ¨ç»†èŠ‚', action='store_true')  # è°ƒè¯•å‰å‘ä¼ æ’­å‚æ•°

    args = parser.parse_args()  # è§£æå‘½ä»¤è¡Œå‚æ•°

    random.seed(args.random_seed)        # è®¾ç½®éšæœºç§å­
    np.random.seed(args.random_seed)     # è®¾ç½®numpyéšæœºç§å­
    torch.manual_seed(args.random_seed)  # è®¾ç½®PyTorchéšæœºç§å­
    torch.cuda.manual_seed(args.random_seed)  # è®¾ç½®CUDAéšæœºç§å­
    torch.cuda.manual_seed_all(args.random_seed)  # è®¾ç½®æ‰€æœ‰CUDAéšæœºç§å­
    torch.backends.cudnn.benchmark = False      # ç¦ç”¨CUDNNåŸºå‡†æµ‹è¯•
    torch.backends.cudnn.deterministic = True   # å¯ç”¨CUDNNç¡®å®šæ€§æ¨¡å¼
    os.environ['PYTHONHASHSEED'] = str(args.random_seed)  # è®¾ç½®Pythonå“ˆå¸Œç§å­

    train_config = {  # å®šä¹‰è®­ç»ƒé…ç½®å­—å…¸
        'batch': args.batch,                    # æ‰¹æ¬¡å¤§å°
        'epoch': args.epoch,                    # è®­ç»ƒè½®æ¬¡
        'slide_win': args.slide_win,            # æ»‘åŠ¨çª—å£å¤§å°
        'dim': args.dim,                        # ç»´åº¦
        'slide_stride': args.slide_stride,      # æ»‘åŠ¨æ­¥é•¿
        'comment': args.comment,                # æ³¨é‡Š
        'seed': args.random_seed,               # éšæœºç§å­
        'out_layer_num': args.out_layer_num,    # è¾“å‡ºå±‚æ•°
        'out_layer_inter_dim': args.out_layer_inter_dim,  # è¾“å‡ºå±‚ä¸­é—´ç»´åº¦
        'decay': args.decay,                    # è¡°å‡
        'val_ratio': args.val_ratio,            # éªŒè¯æ¯”ä¾‹
        'topk': args.topk,                      # topkå‚æ•°
    }

    env_config = {  # å®šä¹‰ç¯å¢ƒé…ç½®å­—å…¸
        'save_path': args.save_path_pattern,    # ä¿å­˜è·¯å¾„
        'dataset': args.dataset,                # æ•°æ®é›†åç§°
        'report': args.report,                  # æŠ¥å‘Šæ¨¡å¼
        'device': args.device,                  # è®¡ç®—è®¾å¤‡
        'load_model_path': args.load_model_path  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    }
    
    # è°ƒè¯•é…ç½®
    debug_config = {  # å®šä¹‰è°ƒè¯•é…ç½®å­—å…¸
        'debug_batch': args.debug_batch,        # è°ƒè¯•æ‰¹æ¬¡é—´éš”
        'debug_forward': args.debug_forward,    # è°ƒè¯•å‰å‘ä¼ æ’­
    }

    main = Main(train_config, env_config, debug=args.debug, debug_config=debug_config)  # åˆ›å»ºMainå®ä¾‹
    main.run()  # è¿è¡Œä¸»ç¨‹åº

    # å…³é—­æ—¥å¿—
    if args.debug:  # å¦‚æœå¼€å¯è°ƒè¯•æ¨¡å¼
        from util.debug_logger import get_logger  # å¯¼å…¥æ—¥å¿—å™¨
        get_logger().close()  # å…³é—­æ—¥å¿—å™¨
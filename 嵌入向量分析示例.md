# åµŒå…¥å‘é‡åˆ†ææŒ‡å—

## ğŸ“‹ æ‚¨å·²ç»ä¿å­˜çš„æ–‡ä»¶

ä¿å­˜åçš„åµŒå…¥å‘é‡æ–‡ä»¶å¯ä»¥ç”¨äºå¤šç§åˆ†æå’Œå¯è§†åŒ–ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å®ç”¨çš„åç»­æ“ä½œ:

---

## ğŸ” ä¸€ã€åŠ è½½å’ŒåŸºæœ¬æŸ¥çœ‹

### 1. åŠ è½½å·²ä¿å­˜çš„åµŒå…¥å‘é‡

```python
import numpy as np

# åŠ è½½.npyæ ¼å¼
embeddings = np.load('embeddings.npy')
print(f"åµŒå…¥å‘é‡å½¢çŠ¶: {embeddings.shape}")  # (27, 64)

# åŠ è½½.csvæ ¼å¼
embeddings = np.loadtxt('embeddings.csv', delimiter=',')

# åŠ è½½.txtæ ¼å¼
embeddings = np.loadtxt('embeddings.txt')
```

### 2. æŸ¥çœ‹ç‰¹å®šèŠ‚ç‚¹çš„åµŒå…¥

```python
# æŸ¥çœ‹èŠ‚ç‚¹0çš„å®Œæ•´åµŒå…¥å‘é‡
node_0_embedding = embeddings[0]
print(f"èŠ‚ç‚¹0çš„åµŒå…¥å‘é‡:\n{node_0_embedding}")

# æŸ¥çœ‹å¤šä¸ªèŠ‚ç‚¹
nodes_of_interest = [0, 5, 10]
for node_id in nodes_of_interest:
    print(f"\nèŠ‚ç‚¹{node_id}: {embeddings[node_id]}")
```

---

## ğŸ“Š äºŒã€å¯è§†åŒ–åˆ†æ

### 1. é™ç»´å¯è§†åŒ– (t-SNE/PCA)

åˆ›å»º `visualize_embeddings.py`:

```python
"""ä½¿ç”¨é™ç»´æŠ€æœ¯å¯è§†åŒ–åµŒå…¥å‘é‡"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# åŠ è½½åµŒå…¥å‘é‡
embeddings = np.load('embeddings.npy')

# 1. ä½¿ç”¨PCAé™åˆ°2ç»´
pca = PCA(n_components=2)
embeddings_2d_pca = pca.fit_transform(embeddings)

# 2. ä½¿ç”¨t-SNEé™åˆ°2ç»´
tsne = TSNE(n_components=2, random_state=42, perplexity=5)
embeddings_2d_tsne = tsne.fit_transform(embeddings)

# ç»˜åˆ¶å¯¹æ¯”å›¾
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# PCAå›¾
axes[0].scatter(embeddings_2d_pca[:, 0], embeddings_2d_pca[:, 1], 
                s=100, alpha=0.6, c=range(len(embeddings)), cmap='tab20')
for i in range(len(embeddings)):
    axes[0].annotate(f'N{i}', (embeddings_2d_pca[i, 0], embeddings_2d_pca[i, 1]),
                     fontsize=9, ha='center')
axes[0].set_title('PCAé™ç»´å¯è§†åŒ– (27ä¸ªèŠ‚ç‚¹)', fontsize=14)
axes[0].set_xlabel('ç¬¬1ä¸»æˆåˆ†', fontsize=12)
axes[0].set_ylabel('ç¬¬2ä¸»æˆåˆ†', fontsize=12)
axes[0].grid(True, alpha=0.3)

# t-SNEå›¾
axes[1].scatter(embeddings_2d_tsne[:, 0], embeddings_2d_tsne[:, 1],
                s=100, alpha=0.6, c=range(len(embeddings)), cmap='tab20')
for i in range(len(embeddings)):
    axes[1].annotate(f'N{i}', (embeddings_2d_tsne[i, 0], embeddings_2d_tsne[i, 1]),
                     fontsize=9, ha='center')
axes[1].set_title('t-SNEé™ç»´å¯è§†åŒ– (27ä¸ªèŠ‚ç‚¹)', fontsize=14)
axes[1].set_xlabel('ç»´åº¦1', fontsize=12)
axes[1].set_ylabel('ç»´åº¦2', fontsize=12)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('embeddings_2d_visualization.png', dpi=300, bbox_inches='tight')
print("âœ“ 2Då¯è§†åŒ–å·²ä¿å­˜: embeddings_2d_visualization.png")
plt.show()
```

### 2. èšç±»åˆ†æ

åˆ›å»º `cluster_embeddings.py`:

```python
"""å¯¹åµŒå…¥å‘é‡è¿›è¡Œèšç±»åˆ†æ"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# åŠ è½½åµŒå…¥å‘é‡
embeddings = np.load('embeddings.npy')

# K-Meansèšç±» (ä¾‹å¦‚åˆ†æˆ3ä¸ªç°‡)
n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels = kmeans.fit_predict(embeddings)

print(f"èšç±»ç»“æœ (å…±{n_clusters}ä¸ªç°‡):")
for cluster_id in range(n_clusters):
    nodes_in_cluster = np.where(cluster_labels == cluster_id)[0]
    print(f"  ç°‡{cluster_id}: èŠ‚ç‚¹ {list(nodes_in_cluster)}")

# ä½¿ç”¨PCAé™ç»´ç”¨äºå¯è§†åŒ–
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(embeddings)

# ç»˜åˆ¶èšç±»ç»“æœ
plt.figure(figsize=(10, 8))
scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                     c=cluster_labels, s=150, alpha=0.6, cmap='viridis')

# æ ‡æ³¨èŠ‚ç‚¹ID
for i in range(len(embeddings)):
    plt.annotate(f'N{i}', (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                fontsize=10, ha='center', fontweight='bold')

# æ ‡æ³¨èšç±»ä¸­å¿ƒ
centers_2d = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers_2d[:, 0], centers_2d[:, 1],
           c='red', s=300, alpha=0.8, marker='X', edgecolors='black', linewidths=2,
           label='èšç±»ä¸­å¿ƒ')

plt.colorbar(scatter, label='ç°‡æ ‡ç­¾')
plt.title(f'èŠ‚ç‚¹åµŒå…¥èšç±»åˆ†æ (K={n_clusters})', fontsize=14)
plt.xlabel('PCAç»´åº¦1', fontsize=12)
plt.ylabel('PCAç»´åº¦2', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('embeddings_clustering.png', dpi=300, bbox_inches='tight')
print("âœ“ èšç±»å¯è§†åŒ–å·²ä¿å­˜: embeddings_clustering.png")
plt.show()
```

---

## ğŸ”— ä¸‰ã€ç›¸ä¼¼åº¦åˆ†æ

### 1. è®¡ç®—èŠ‚ç‚¹é—´çš„è·ç¦»çŸ©é˜µ

åˆ›å»º `analyze_similarity.py`:

```python
"""åˆ†æèŠ‚ç‚¹é—´çš„ç›¸ä¼¼åº¦å’Œè·ç¦»"""
import numpy as np
from scipy.spatial.distance import pdist, squareform
import matplotlib.pyplot as plt
import seaborn as sns

# åŠ è½½åµŒå…¥å‘é‡
embeddings = np.load('embeddings.npy')

# 1. æ¬§æ°è·ç¦»çŸ©é˜µ
euclidean_dist = squareform(pdist(embeddings, metric='euclidean'))

# 2. ä½™å¼¦è·ç¦»çŸ©é˜µ (1 - ä½™å¼¦ç›¸ä¼¼åº¦)
cosine_dist = squareform(pdist(embeddings, metric='cosine'))

# 3. ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
cosine_similarity = 1 - cosine_dist

# ç»˜åˆ¶ä¸‰ç§çŸ©é˜µå¯¹æ¯”
fig, axes = plt.subplots(1, 3, figsize=(20, 6))

# æ¬§æ°è·ç¦»
sns.heatmap(euclidean_dist, ax=axes[0], cmap='YlOrRd', square=True,
            cbar_kws={"label": "æ¬§æ°è·ç¦»"})
axes[0].set_title('æ¬§æ°è·ç¦»çŸ©é˜µ', fontsize=14)

# ä½™å¼¦è·ç¦»
sns.heatmap(cosine_dist, ax=axes[1], cmap='YlOrRd', square=True,
            cbar_kws={"label": "ä½™å¼¦è·ç¦»"})
axes[1].set_title('ä½™å¼¦è·ç¦»çŸ©é˜µ', fontsize=14)

# ä½™å¼¦ç›¸ä¼¼åº¦
sns.heatmap(cosine_similarity, ax=axes[2], cmap='RdBu_r', center=0, square=True,
            cbar_kws={"label": "ä½™å¼¦ç›¸ä¼¼åº¦"}, vmin=-1, vmax=1)
axes[2].set_title('ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ', fontsize=14)

plt.tight_layout()
plt.savefig('distance_similarity_matrices.png', dpi=300, bbox_inches='tight')
print("âœ“ è·ç¦»å’Œç›¸ä¼¼åº¦çŸ©é˜µå·²ä¿å­˜")
plt.show()

# æ‰¾å‡ºæœ€ç›¸ä¼¼çš„èŠ‚ç‚¹å¯¹(ä½™å¼¦ç›¸ä¼¼åº¦)
n_pairs = 10
similarity_triu = np.triu(cosine_similarity, k=1)  # åªçœ‹ä¸Šä¸‰è§’
flat_indices = np.argsort(similarity_triu.flatten())[::-1][:n_pairs]
node_pairs = np.unravel_index(flat_indices, similarity_triu.shape)

print(f"\nå‰{n_pairs}ä¸ªæœ€ç›¸ä¼¼çš„èŠ‚ç‚¹å¯¹:")
for i, (node1, node2) in enumerate(zip(node_pairs[0], node_pairs[1])):
    sim = cosine_similarity[node1, node2]
    print(f"{i+1:2d}. èŠ‚ç‚¹{node1:2d} â†” èŠ‚ç‚¹{node2:2d}  |  ç›¸ä¼¼åº¦: {sim:.6f}")
```

### 2. æŸ¥æ‰¾æ¯ä¸ªèŠ‚ç‚¹çš„æœ€ç›¸ä¼¼é‚»å±…

```python
"""æ‰¾å‡ºæ¯ä¸ªèŠ‚ç‚¹æœ€ç›¸ä¼¼çš„Kä¸ªé‚»å±…"""
import numpy as np

embeddings = np.load('embeddings.npy')

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
similarity_matrix = cosine_similarity(embeddings)

k = 5  # æ‰¾å‰5ä¸ªæœ€ç›¸ä¼¼çš„é‚»å±…
print(f"æ¯ä¸ªèŠ‚ç‚¹çš„å‰{k}ä¸ªæœ€ç›¸ä¼¼é‚»å±…:\n")

for node_id in range(len(embeddings)):
    # è·å–ç›¸ä¼¼åº¦å¹¶æ’åº(æ’é™¤è‡ªå·±)
    similarities = similarity_matrix[node_id].copy()
    similarities[node_id] = -np.inf  # æ’é™¤è‡ªå·±
    
    top_k_indices = np.argsort(similarities)[::-1][:k]
    
    print(f"èŠ‚ç‚¹{node_id:2d}çš„é‚»å±…:", end=" ")
    for neighbor in top_k_indices:
        sim = similarity_matrix[node_id, neighbor]
        print(f"N{neighbor}({sim:.3f})", end=" ")
    print()
```

---

## ğŸ“ˆ å››ã€ç»Ÿè®¡åˆ†æ

åˆ›å»º `statistical_analysis.py`:

```python
"""å¯¹åµŒå…¥å‘é‡è¿›è¡Œç»Ÿè®¡åˆ†æ"""
import numpy as np
import matplotlib.pyplot as plt

embeddings = np.load('embeddings.npy')

print("="*80)
print("åµŒå…¥å‘é‡ç»Ÿè®¡åˆ†æ")
print("="*80)

# 1. æ¯ä¸ªèŠ‚ç‚¹çš„èŒƒæ•°åˆ†å¸ƒ
norms = np.linalg.norm(embeddings, axis=1)
print(f"\nL2èŒƒæ•°ç»Ÿè®¡:")
print(f"  æœ€å°: {norms.min():.6f}  (èŠ‚ç‚¹{norms.argmin()})")
print(f"  æœ€å¤§: {norms.max():.6f}  (èŠ‚ç‚¹{norms.argmax()})")
print(f"  å¹³å‡: {norms.mean():.6f}")
print(f"  æ ‡å‡†å·®: {norms.std():.6f}")

# 2. æ¯ä¸ªç»´åº¦çš„é‡è¦æ€§
dim_variance = np.var(embeddings, axis=0)
top_dims = np.argsort(dim_variance)[::-1][:10]

print(f"\nå‰10ä¸ªæœ€é‡è¦çš„ç»´åº¦(æŒ‰æ–¹å·®æ’åº):")
for i, dim in enumerate(top_dims):
    print(f"{i+1:2d}. ç»´åº¦{dim:2d}: æ–¹å·®={dim_variance[dim]:.6f}")

# 3. å¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# L2èŒƒæ•°åˆ†å¸ƒ
axes[0, 0].bar(range(len(norms)), norms, alpha=0.7, color='steelblue')
axes[0, 0].set_xlabel('èŠ‚ç‚¹ID', fontsize=11)
axes[0, 0].set_ylabel('L2èŒƒæ•°', fontsize=11)
axes[0, 0].set_title('æ¯ä¸ªèŠ‚ç‚¹çš„L2èŒƒæ•°', fontsize=12)
axes[0, 0].grid(True, alpha=0.3)

# èŒƒæ•°ç›´æ–¹å›¾
axes[0, 1].hist(norms, bins=15, alpha=0.7, color='coral', edgecolor='black')
axes[0, 1].set_xlabel('L2èŒƒæ•°', fontsize=11)
axes[0, 1].set_ylabel('é¢‘æ•°', fontsize=11)
axes[0, 1].set_title('L2èŒƒæ•°åˆ†å¸ƒç›´æ–¹å›¾', fontsize=12)
axes[0, 1].grid(True, alpha=0.3)

# ç»´åº¦æ–¹å·®
axes[1, 0].bar(range(len(dim_variance)), dim_variance, alpha=0.7, color='green')
axes[1, 0].set_xlabel('ç»´åº¦', fontsize=11)
axes[1, 0].set_ylabel('æ–¹å·®', fontsize=11)
axes[1, 0].set_title('æ¯ä¸ªç»´åº¦çš„æ–¹å·®', fontsize=12)
axes[1, 0].grid(True, alpha=0.3)

# åµŒå…¥å€¼åˆ†å¸ƒ
axes[1, 1].hist(embeddings.flatten(), bins=50, alpha=0.7, color='purple', edgecolor='black')
axes[1, 1].set_xlabel('åµŒå…¥å€¼', fontsize=11)
axes[1, 1].set_ylabel('é¢‘æ•°', fontsize=11)
axes[1, 1].set_title('æ‰€æœ‰åµŒå…¥å€¼çš„åˆ†å¸ƒ', fontsize=12)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('embeddings_statistics.png', dpi=300, bbox_inches='tight')
print("\nâœ“ ç»Ÿè®¡å›¾è¡¨å·²ä¿å­˜: embeddings_statistics.png")
plt.show()
```

---

## ğŸ¯ äº”ã€ä¸å®é™…ä»»åŠ¡ç»“åˆ

### 1. åˆ†æèŠ‚ç‚¹é‡è¦æ€§

```python
"""åŸºäºåµŒå…¥å‘é‡åˆ†æèŠ‚ç‚¹é‡è¦æ€§"""
import numpy as np

embeddings = np.load('embeddings.npy')

# æ–¹æ³•1: åŸºäºL2èŒƒæ•°
norms = np.linalg.norm(embeddings, axis=1)
importance_norm = norms / norms.sum()

# æ–¹æ³•2: åŸºäºä¸å…¶ä»–èŠ‚ç‚¹çš„å¹³å‡ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
similarity_matrix = cosine_similarity(embeddings)
# æ’é™¤å¯¹è§’çº¿
np.fill_diagonal(similarity_matrix, 0)
importance_sim = similarity_matrix.sum(axis=1) / (len(embeddings) - 1)

# æ–¹æ³•3: åŸºäºç»´åº¦æ–¹å·®è´¡çŒ®
dim_variance = np.var(embeddings, axis=0)
importance_var = np.abs(embeddings) @ dim_variance

print("èŠ‚ç‚¹é‡è¦æ€§æ’å:\n")
print(f"{'æ’å':<6} {'èŠ‚ç‚¹':<6} {'èŒƒæ•°é‡è¦æ€§':<15} {'ç›¸ä¼¼åº¦é‡è¦æ€§':<15} {'æ–¹å·®é‡è¦æ€§':<15}")
print("-" * 70)

# ç»¼åˆæ’å(å¹³å‡ä¸‰ç§æ–¹æ³•)
importance_combined = (
    importance_norm / importance_norm.max() +
    importance_sim / importance_sim.max() +
    importance_var / importance_var.max()
) / 3

ranking = np.argsort(importance_combined)[::-1]

for rank, node_id in enumerate(ranking[:10]):
    print(f"{rank+1:<6} {node_id:<6} {importance_norm[node_id]:<15.6f} "
          f"{importance_sim[node_id]:<15.6f} {importance_var[node_id]:<15.6f}")
```

### 2. å¯¼å‡ºä¸ºå›¾ç»“æ„

```python
"""å°†åµŒå…¥å‘é‡çš„ç›¸ä¼¼åº¦è½¬æ¢ä¸ºå›¾ç»“æ„"""
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity

embeddings = np.load('embeddings.npy')
similarity_matrix = cosine_similarity(embeddings)

# åˆ›å»ºå›¾:ä»…ä¿ç•™ç›¸ä¼¼åº¦>é˜ˆå€¼çš„è¾¹
threshold = 0.5
G = nx.Graph()

for i in range(len(embeddings)):
    for j in range(i+1, len(embeddings)):
        if similarity_matrix[i, j] > threshold:
            G.add_edge(i, j, weight=similarity_matrix[i, j])

print(f"å›¾ç»Ÿè®¡ä¿¡æ¯ (é˜ˆå€¼={threshold}):")
print(f"  èŠ‚ç‚¹æ•°: {G.number_of_nodes()}")
print(f"  è¾¹æ•°: {G.number_of_edges()}")
print(f"  å¹³å‡åº¦: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}")

# å¯è§†åŒ–å›¾
plt.figure(figsize=(12, 12))
pos = nx.spring_layout(G, k=1, iterations=50)
nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue', 
                       edgecolors='black', linewidths=1.5)
nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')
edges = G.edges()
weights = [G[u][v]['weight'] for u, v in edges]
nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights], alpha=0.5)

plt.title(f'åŸºäºåµŒå…¥ç›¸ä¼¼åº¦çš„å›¾ç»“æ„ (é˜ˆå€¼={threshold})', fontsize=14)
plt.axis('off')
plt.tight_layout()
plt.savefig('embedding_graph.png', dpi=300, bbox_inches='tight')
print("âœ“ å›¾ç»“æ„å·²ä¿å­˜: embedding_graph.png")
plt.show()
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

1. **é™ç»´å¯è§†åŒ–** - æœ€ç›´è§‚çš„åˆ†ææ–¹å¼
   ```bash
   python visualize_embeddings.py
   ```

2. **èšç±»åˆ†æ** - å‘ç°èŠ‚ç‚¹åˆ†ç»„
   ```bash
   python cluster_embeddings.py
   ```

3. **ç›¸ä¼¼åº¦åˆ†æ** - æ‰¾å‡ºèŠ‚ç‚¹å…³ç³»
   ```bash
   python analyze_similarity.py
   ```

4. **ç»Ÿè®¡åˆ†æ** - ç†è§£åµŒå…¥åˆ†å¸ƒ
   ```bash
   python statistical_analysis.py
   ```

---

## ğŸ’¡ å®ç”¨å»ºè®®

1. **å…ˆåšé™ç»´å¯è§†åŒ–**,ç›´è§‚ç†è§£èŠ‚ç‚¹åˆ†å¸ƒ
2. **ç„¶ååšèšç±»åˆ†æ**,å‘ç°æ½œåœ¨çš„èŠ‚ç‚¹ç»„
3. **æ¥ç€åˆ†æç›¸ä¼¼åº¦**,ç†è§£èŠ‚ç‚¹é—´å…³ç³»
4. **æœ€åç»“åˆä¸šåŠ¡çŸ¥è¯†**,è§£é‡Šå‘ç°çš„æ¨¡å¼

---

## ğŸ“¦ æ‰€éœ€ä¾èµ–

```bash
pip install numpy matplotlib seaborn scikit-learn scipy networkx
```

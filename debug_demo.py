# -*- coding: utf-8 -*-
"""
å¿«é€Ÿè°ƒè¯•æ¼”ç¤ºè„šæœ¬
æ¼”ç¤ºå¦‚ä½•åœ¨GDNè®­ç»ƒä¸­ä½¿ç”¨è°ƒè¯•æ–­ç‚¹æŸ¥çœ‹æ•°æ®å˜åŒ–
"""

# åœ¨ train.py ä¸­çš„ç¬¬112è¡Œåæ·»åŠ ä»¥ä¸‹ä»£ç ï¼ˆè®­ç»ƒå¾ªç¯å¼€å§‹å¤„ï¼‰

"""
===========================================
æ–¹æ³•1: ä½¿ç”¨ pdb è°ƒè¯•å™¨ï¼ˆæœ€ç®€å•ï¼‰
===========================================
"""

# æ·»åŠ åœ¨ train.py ç¬¬112è¡Œä¹‹åï¼š
'''
    for i_epoch in range(epoch):
        epoch_start_time = time.time()
        
        # ğŸ”¥ æ·»åŠ æ–­ç‚¹ï¼šåªåœ¨ç¬¬1ä¸ªepochçš„ç¬¬1ä¸ªbatchæš‚åœ
        if i_epoch == 0:
            import pdb; pdb.set_trace()
        
        logger.log_epoch_start(i_epoch, epoch)
'''

# ç„¶åè¿è¡Œï¼š python main.py -dataset swat -epoch 5
# ç¨‹åºä¼šåœ¨æ–­ç‚¹å¤„æš‚åœï¼Œä½ å¯ä»¥è¾“å…¥ï¼š
#   p x.shape          # æŸ¥çœ‹è¾“å…¥shape
#   p out.shape        # æŸ¥çœ‹è¾“å‡ºshape  
#   p loss.item()      # æŸ¥çœ‹æŸå¤±å€¼
#   c                  # ç»§ç»­è¿è¡Œ


"""
===========================================
æ–¹æ³•2: æ·»åŠ è¯¦ç»†æ‰“å°ï¼ˆæ¨èï¼‰
===========================================
"""

# åœ¨ train.py ç¬¬125-130è¡Œï¼ˆæ¨¡å‹å‰å‘ä¼ æ’­å¤„ï¼‰æ·»åŠ ï¼š
'''
            optimizer.zero_grad()
            out = model(x, edge_index).float().to(device)
            loss = loss_func(out, labels)
            
            # ğŸ”¥ æ·»åŠ è¯¦ç»†æ‰“å°
            if batch_idx == 0 and i_epoch % 5 == 0:  # æ¯5ä¸ªepochæ‰“å°ä¸€æ¬¡
                print("\n" + "="*70)
                print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ [Epoch {i_epoch}, Batch {batch_idx}]")
                print("="*70)
                print(f"  ğŸ“¥ è¾“å…¥ x:")
                print(f"     Shape: {x.shape}")
                print(f"     Range: [{x.min():.4f}, {x.max():.4f}]")
                print(f"     Mean:  {x.mean():.4f} Â± {x.std():.4f}")
                print(f"\n  ğŸ“¤ è¾“å‡º out:")
                print(f"     Shape: {out.shape}")
                print(f"     Range: [{out.min():.4f}, {out.max():.4f}]")
                print(f"     Mean:  {out.mean():.4f} Â± {out.std():.4f}")
                print(f"\n  ğŸ¯ æ ‡ç­¾ labels:")
                print(f"     Shape: {labels.shape}")
                print(f"     Range: [{labels.min():.4f}, {labels.max():.4f}]")
                print(f"     Mean:  {labels.mean():.4f} Â± {labels.std():.4f}")
                print(f"\n  ğŸ“Š æŸå¤±:")
                print(f"     MSE Loss: {loss.item():.6f}")
                
                # è®¡ç®—æ¯ä¸ªä¼ æ„Ÿå™¨çš„é¢„æµ‹è¯¯å·®
                errors = (out - labels).abs().mean(dim=0)
                print(f"\n  ğŸ“ˆ å„ä¼ æ„Ÿå™¨é¢„æµ‹è¯¯å·®:")
                print(f"     å¹³å‡è¯¯å·®: {errors.mean():.4f}")
                print(f"     æœ€å¤§è¯¯å·®: {errors.max():.4f} (ä¼ æ„Ÿå™¨ {errors.argmax()})")
                print(f"     æœ€å°è¯¯å·®: {errors.min():.4f} (ä¼ æ„Ÿå™¨ {errors.argmin()})")
                print("="*70 + "\n")
            
            loss.backward()
'''


"""
===========================================
æ–¹æ³•3: ä½¿ç”¨debug_utilså·¥å…·ï¼ˆåŠŸèƒ½æœ€å¼ºï¼‰
===========================================
"""

# åœ¨ train.py é¡¶éƒ¨å¯¼å…¥ï¼š
'''
from util.debug_utils import print_tensor_stats, plot_batch_distribution, check_gradients
'''

# åœ¨ train.py ç¬¬125-140è¡Œæ·»åŠ ï¼š
'''
            optimizer.zero_grad()
            out = model(x, edge_index).float().to(device)
            loss = loss_func(out, labels)
            
            # ğŸ”¥ ä½¿ç”¨è°ƒè¯•å·¥å…·
            if batch_idx == 0 and i_epoch == 0:  # ç¬¬ä¸€ä¸ªepochçš„ç¬¬ä¸€ä¸ªbatch
                from util.debug_utils import print_tensor_stats, plot_batch_distribution
                
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                print_tensor_stats(x, "è¾“å…¥x", show_values=False)
                print_tensor_stats(out, "æ¨¡å‹è¾“å‡º", show_values=False)
                print_tensor_stats(labels, "çœŸå®æ ‡ç­¾", show_values=False)
                
                # å¯è§†åŒ–åˆ†å¸ƒ
                plot_batch_distribution(x, "ç¬¬0ä¸ªepochè¾“å…¥åˆ†å¸ƒ")
                plot_batch_distribution(out.unsqueeze(-1), "ç¬¬0ä¸ªepochè¾“å‡ºåˆ†å¸ƒ")
            
            loss.backward()
            
            # ğŸ”¥ æ£€æŸ¥æ¢¯åº¦
            if batch_idx == 0 and i_epoch == 0:
                from util.debug_utils import check_gradients
                grad_info = check_gradients(model, threshold=10.0)
                if grad_info['has_nan']:
                    print("âš ï¸ å‘ç°NaNæ¢¯åº¦ï¼Œè¯·æ£€æŸ¥ï¼")
            
            optimizer.step()
'''


"""
===========================================
æ–¹æ³•4: ä½¿ç”¨VS Codeè°ƒè¯•å™¨ï¼ˆå¯è§†åŒ–æœ€å¥½ï¼‰
===========================================
"""

# æ­¥éª¤ï¼š
# 1. åœ¨VS Codeä¸­æ‰“å¼€ train.py
# 2. åœ¨ç¬¬125è¡Œï¼ˆout = model(x, edge_index)ï¼‰å·¦ä¾§ç‚¹å‡»ï¼Œæ·»åŠ çº¢è‰²æ–­ç‚¹
# 3. æŒ‰F5ï¼Œé€‰æ‹© "ğŸ› è°ƒè¯•GDN (å°æ•°æ®é›†)"
# 4. ç¨‹åºè¿è¡Œåˆ°æ–­ç‚¹å¤„ä¼šæš‚åœ
# 5. åœ¨å·¦ä¾§"å˜é‡"é¢æ¿æŸ¥çœ‹æ‰€æœ‰å˜é‡
# 6. åœ¨"è°ƒè¯•æ§åˆ¶å°"è¾“å…¥è¡¨è¾¾å¼ï¼š
#    - x.shape
#    - out.mean()
#    - loss.item()
# 7. æŒ‰F5ç»§ç»­ï¼Œæˆ–F10å•æ­¥æ‰§è¡Œ


"""
===========================================
æ¨èä½¿ç”¨æµç¨‹
===========================================
"""

print("""
ğŸ“– æ¨èçš„è°ƒè¯•æµç¨‹ï¼š

1ï¸âƒ£ ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆäº†è§£æ•´ä½“æµç¨‹ï¼‰ï¼š
   python main.py -dataset swat -epoch 2 -batch 32 --debug --debug_batch 1
   â†’ æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶äº†è§£æ•°æ®æµå‘

2ï¸âƒ£ å‘ç°é—®é¢˜ï¼ˆæ·±å…¥è°ƒè¯•ï¼‰ï¼š
   - åœ¨train.pyä¸­éœ€è¦æ£€æŸ¥çš„ä½ç½®æ·»åŠ æ‰“å°è¯­å¥
   - æˆ–ä½¿ç”¨VS Codeæ–­ç‚¹è°ƒè¯•
   
3ï¸âƒ£ æ’æŸ¥æ•°æ®å¼‚å¸¸ï¼š
   - ä½¿ç”¨ debug_utils.py ä¸­çš„å·¥å…·å‡½æ•°
   - æ‰“å°ç»Ÿè®¡ä¿¡æ¯
   - ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡

4ï¸âƒ£ æ£€æŸ¥æ¢¯åº¦é—®é¢˜ï¼š
   - ä½¿ç”¨ check_gradients() å‡½æ•°
   - æŸ¥çœ‹æ˜¯å¦æœ‰æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±

ğŸ’¡ æç¤ºï¼š
   - å¯¹äºå¿«é€ŸéªŒè¯ï¼šç›´æ¥ç”¨ print()
   - å¯¹äºæ·±å…¥åˆ†æï¼šä½¿ç”¨ VS Code æ–­ç‚¹
   - å¯¹äºæŒç»­ç›‘æ§ï¼šå¯ç”¨ --debug æ—¥å¿—
""")


"""
===========================================
å®æˆ˜ä¾‹å­ï¼šè¿½è¸ªç¬¬ä¸€ä¸ªbatchçš„æ•°æ®æµ
===========================================
"""

# å®Œæ•´ç¤ºä¾‹ä»£ç ï¼ˆæ·»åŠ åˆ° train.pyï¼‰:
'''
def train(...):
    # ... å‰é¢çš„ä»£ç  ...
    
    for i_epoch in range(epoch):
        for batch_idx, (x, labels, attack_labels, edge_index) in enumerate(dataloader):
            
            # ç§»åŠ¨åˆ°GPU
            x, labels, edge_index = [item.float().to(device) for item in [x, labels, edge_index]]
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ è°ƒè¯•ä»£ç å¼€å§‹ ğŸ”¥ğŸ”¥ğŸ”¥
            if batch_idx == 0 and i_epoch == 0:
                print("\n" + "ğŸ”"*40)
                print("ã€æ•°æ®æµè¿½è¸ªã€‘ç¬¬0ä¸ªepochï¼Œç¬¬0ä¸ªbatch")
                print("ğŸ”"*40)
                
                # 1. æŸ¥çœ‹åŸå§‹è¾“å…¥
                print(f"\n1ï¸âƒ£ åŸå§‹è¾“å…¥æ•°æ®:")
                print(f"   x.shape = {x.shape}  # [batch=32, sensors=38, time_steps=15]")
                print(f"   xçš„å–å€¼èŒƒå›´: [{x.min():.3f}, {x.max():.3f}]")
                print(f"   ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªä¼ æ„Ÿå™¨çš„æ—¶é—´åºåˆ—: {x[0, 0, :]}")
                
                # 2. æ¨¡å‹å‰å‘ä¼ æ’­
                print(f"\n2ï¸âƒ£ æ¨¡å‹å‰å‘ä¼ æ’­...")
                
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            out = model(x, edge_index).float().to(device)
            
            if batch_idx == 0 and i_epoch == 0:
                # 3. æŸ¥çœ‹æ¨¡å‹è¾“å‡º
                print(f"\n3ï¸âƒ£ æ¨¡å‹è¾“å‡º:")
                print(f"   out.shape = {out.shape}  # [batch=32, sensors=38]")
                print(f"   outçš„å–å€¼èŒƒå›´: [{out.min():.3f}, {out.max():.3f}]")
                print(f"   ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é¢„æµ‹: {out[0, :5]}  # å‰5ä¸ªä¼ æ„Ÿå™¨")
                
                # 4. æŸ¥çœ‹çœŸå®æ ‡ç­¾
                print(f"\n4ï¸âƒ£ çœŸå®æ ‡ç­¾:")
                print(f"   labels.shape = {labels.shape}  # [batch=32, sensors=38]")
                print(f"   ç¬¬ä¸€ä¸ªæ ·æœ¬çš„çœŸå€¼: {labels[0, :5]}  # å‰5ä¸ªä¼ æ„Ÿå™¨")
                
            # è®¡ç®—æŸå¤±
            loss = loss_func(out, labels)
            
            if batch_idx == 0 and i_epoch == 0:
                # 5. æŸ¥çœ‹æŸå¤±
                print(f"\n5ï¸âƒ£ æŸå¤±è®¡ç®—:")
                print(f"   MSE Loss = {loss.item():.6f}")
                
                # 6. æ‰‹åŠ¨è®¡ç®—ç¬¬ä¸€ä¸ªæ ·æœ¬çš„MSEéªŒè¯
                sample_mse = ((out[0] - labels[0]) ** 2).mean().item()
                print(f"   ç¬¬ä¸€ä¸ªæ ·æœ¬çš„MSE (éªŒè¯) = {sample_mse:.6f}")
                
            # åå‘ä¼ æ’­
            loss.backward()
            
            if batch_idx == 0 and i_epoch == 0:
                # 7. æŸ¥çœ‹æ¢¯åº¦
                print(f"\n6ï¸âƒ£ æ¢¯åº¦ä¿¡æ¯:")
                first_param = next(model.parameters())
                if first_param.grad is not None:
                    print(f"   ç¬¬ä¸€å±‚å‚æ•°çš„æ¢¯åº¦èŒƒå›´: [{first_param.grad.min():.6f}, {first_param.grad.max():.6f}]")
                    print(f"   æ¢¯åº¦çš„å¹³å‡å€¼: {first_param.grad.mean():.6f}")
                else:
                    print(f"   âš ï¸ æ¢¯åº¦ä¸ºNone")
                
                print("\n" + "ğŸ”"*40 + "\n")
                
                # å¯é€‰ï¼šåœ¨è¿™é‡Œè®¾ç½®æ–­ç‚¹æš‚åœ
                # import pdb; pdb.set_trace()
            
            # ğŸ”¥ğŸ”¥ğŸ”¥ è°ƒè¯•ä»£ç ç»“æŸ ğŸ”¥ğŸ”¥ğŸ”¥
            
            optimizer.step()
            # ... åç»­ä»£ç  ...
'''

print("\nâœ… è°ƒè¯•æ¼”ç¤ºè„šæœ¬å‡†å¤‡å®Œæˆï¼")
print("ğŸ“ è¯·æ ¹æ®ä¸Šé¢çš„ç¤ºä¾‹ä»£ç ä¿®æ”¹æ‚¨çš„ train.py æ–‡ä»¶")

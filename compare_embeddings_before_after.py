# -*- coding: utf-8 -*-
"""
å¯¹æ¯”è®­ç»ƒå‰åçš„åµŒå…¥å‘é‡
å±•ç¤ºæ¨¡å‹å­¦ä¹ çš„æ•ˆæœ
"""

import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
from models.GDN import GDN
from util.env import get_device

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

print("="*80)
print("å¯¹æ¯”è®­ç»ƒå‰åçš„åµŒå…¥å‘é‡")
print("="*80)

# å‚æ•°è®¾ç½®
MODEL_PATH = 'pretrained/msl/best_01_07-154250.pt'
NODE_NUM = 27
DIM = 64
TOPK = 20

device = get_device()

# 1. åˆ›å»ºæœªè®­ç»ƒçš„æ¨¡å‹(éšæœºåˆå§‹åŒ–)
print("\nğŸ“‚ åˆ›å»ºéšæœºåˆå§‹åŒ–çš„æ¨¡å‹(è®­ç»ƒå‰)...")
edge_index = torch.zeros((2, NODE_NUM * TOPK), dtype=torch.long)
model_before = GDN(
    edge_index_sets=[edge_index],
    node_num=NODE_NUM,
    dim=DIM,
    input_dim=15,
    topk=TOPK
).to(device)

model_before.eval()
with torch.no_grad():
    embeddings_before = model_before.embedding.weight.cpu().numpy()

print(f"âœ“ è®­ç»ƒå‰åµŒå…¥å‘é‡: {embeddings_before.shape}")

# 2. åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
print("\nğŸ“‚ åŠ è½½è®­ç»ƒåçš„æ¨¡å‹...")
model_after = GDN(
    edge_index_sets=[edge_index],
    node_num=NODE_NUM,
    dim=DIM,
    input_dim=15,
    topk=TOPK
).to(device)

model_after.load_state_dict(torch.load(MODEL_PATH, map_location=device))
model_after.eval()

with torch.no_grad():
    embeddings_after = model_after.embedding.weight.cpu().numpy()

print(f"âœ“ è®­ç»ƒååµŒå…¥å‘é‡: {embeddings_after.shape}")

# 3. ç»Ÿè®¡å¯¹æ¯”
print("\n" + "="*80)
print("ç»Ÿè®¡å¯¹æ¯”")
print("="*80)

print("\nã€è®­ç»ƒå‰ - éšæœºåˆå§‹åŒ–ã€‘")
print(f"  æœ€å°å€¼: {embeddings_before.min():.6f}")
print(f"  æœ€å¤§å€¼: {embeddings_before.max():.6f}")
print(f"  å¹³å‡å€¼: {embeddings_before.mean():.6f}")
print(f"  æ ‡å‡†å·®: {embeddings_before.std():.6f}")

norms_before = np.linalg.norm(embeddings_before, axis=1)
print(f"  å¹³å‡L2èŒƒæ•°: {norms_before.mean():.6f}")

print("\nã€è®­ç»ƒå - å­¦ä¹ å¾—åˆ°ã€‘")
print(f"  æœ€å°å€¼: {embeddings_after.min():.6f}")
print(f"  æœ€å¤§å€¼: {embeddings_after.max():.6f}")
print(f"  å¹³å‡å€¼: {embeddings_after.mean():.6f}")
print(f"  æ ‡å‡†å·®: {embeddings_after.std():.6f}")

norms_after = np.linalg.norm(embeddings_after, axis=1)
print(f"  å¹³å‡L2èŒƒæ•°: {norms_after.mean():.6f}")

# 4. ç›¸ä¼¼åº¦å¯¹æ¯”
print("\n" + "="*80)
print("èŠ‚ç‚¹ç›¸ä¼¼åº¦ç»“æ„å¯¹æ¯”")
print("="*80)

cos_sim_before = cosine_similarity(embeddings_before)
cos_sim_after = cosine_similarity(embeddings_after)

# æ’é™¤å¯¹è§’çº¿
np.fill_diagonal(cos_sim_before, 0)
np.fill_diagonal(cos_sim_after, 0)

print("\nã€è®­ç»ƒå‰ã€‘èŠ‚ç‚¹é—´ä½™å¼¦ç›¸ä¼¼åº¦:")
print(f"  å¹³å‡ç›¸ä¼¼åº¦: {cos_sim_before.mean():.6f}")
print(f"  æœ€å¤§ç›¸ä¼¼åº¦: {cos_sim_before.max():.6f}")
print(f"  æœ€å°ç›¸ä¼¼åº¦: {cos_sim_before.min():.6f}")
print(f"  æ ‡å‡†å·®: {cos_sim_before.std():.6f}")

print("\nã€è®­ç»ƒåã€‘èŠ‚ç‚¹é—´ä½™å¼¦ç›¸ä¼¼åº¦:")
print(f"  å¹³å‡ç›¸ä¼¼åº¦: {cos_sim_after.mean():.6f}")
print(f"  æœ€å¤§ç›¸ä¼¼åº¦: {cos_sim_after.max():.6f}")
print(f"  æœ€å°ç›¸ä¼¼åº¦: {cos_sim_after.min():.6f}")
print(f"  æ ‡å‡†å·®: {cos_sim_after.std():.6f}")

print(f"\nğŸ’¡ ç›¸ä¼¼åº¦æ ‡å‡†å·®å˜åŒ–: {cos_sim_before.std():.6f} â†’ {cos_sim_after.std():.6f}")
if cos_sim_after.std() > cos_sim_before.std():
    print("   âœ“ è®­ç»ƒåèŠ‚ç‚¹é—´çš„å·®å¼‚æ€§å¢å¼º,æ¨¡å‹å­¦ä¹ åˆ°äº†æ›´æ˜ç¡®çš„èŠ‚ç‚¹å…³ç³»!")
else:
    print("   ç›¸ä¼¼åº¦åˆ†å¸ƒå˜åŒ–è¾ƒå°")

# 5. å¯è§†åŒ–å¯¹æ¯”
print("\nğŸ¨ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")

fig = plt.figure(figsize=(18, 7))

# è®­ç»ƒå‰ - PCA
pca_before = PCA(n_components=2)
emb_2d_before = pca_before.fit_transform(embeddings_before)

ax1 = plt.subplot(1, 3, 1)
scatter1 = ax1.scatter(emb_2d_before[:, 0], emb_2d_before[:, 1],
                       s=150, alpha=0.7, c=range(NODE_NUM),
                       cmap='tab20', edgecolors='black', linewidths=1.5)
for i in range(NODE_NUM):
    ax1.annotate(f'{i}', (emb_2d_before[i, 0], emb_2d_before[i, 1]),
                fontsize=9, ha='center', va='center', fontweight='bold')
ax1.set_title('è®­ç»ƒå‰ (éšæœºåˆå§‹åŒ–)\nèŠ‚ç‚¹åˆ†å¸ƒè¾ƒå‡åŒ€', fontsize=12, fontweight='bold')
ax1.set_xlabel('PCAç»´åº¦1', fontsize=10)
ax1.set_ylabel('PCAç»´åº¦2', fontsize=10)
ax1.grid(True, alpha=0.3)

# è®­ç»ƒå - PCA
pca_after = PCA(n_components=2)
emb_2d_after = pca_after.fit_transform(embeddings_after)

ax2 = plt.subplot(1, 3, 2)
scatter2 = ax2.scatter(emb_2d_after[:, 0], emb_2d_after[:, 1],
                       s=150, alpha=0.7, c=range(NODE_NUM),
                       cmap='tab20', edgecolors='black', linewidths=1.5)
for i in range(NODE_NUM):
    ax2.annotate(f'{i}', (emb_2d_after[i, 0], emb_2d_after[i, 1]),
                fontsize=9, ha='center', va='center', fontweight='bold')
ax2.set_title('è®­ç»ƒå (å­¦ä¹ å¾—åˆ°)\nèŠ‚ç‚¹å½¢æˆæ˜æ˜¾çš„èšç±»', fontsize=12, fontweight='bold', color='green')
ax2.set_xlabel('PCAç»´åº¦1', fontsize=10)
ax2.set_ylabel('PCAç»´åº¦2', fontsize=10)
ax2.grid(True, alpha=0.3)

# ç›¸ä¼¼åº¦çŸ©é˜µå¯¹æ¯”
ax3 = plt.subplot(1, 3, 3)
diff = cos_sim_after - cos_sim_before
im = ax3.imshow(diff, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
ax3.set_title('ç›¸ä¼¼åº¦å˜åŒ–\n(è®­ç»ƒå - è®­ç»ƒå‰)', fontsize=12, fontweight='bold')
ax3.set_xlabel('èŠ‚ç‚¹', fontsize=10)
ax3.set_ylabel('èŠ‚ç‚¹', fontsize=10)
plt.colorbar(im, ax=ax3, label='ç›¸ä¼¼åº¦å˜åŒ–')

plt.tight_layout()
plt.savefig('embeddings_before_after_comparison.png', dpi=300, bbox_inches='tight')
print("âœ“ å¯¹æ¯”å›¾å·²ä¿å­˜: embeddings_before_after_comparison.png")

# 6. æ‰¾å‡ºå­¦ä¹ åˆ°çš„æœ€å¼ºå…³ç³»
print("\n" + "="*80)
print("æ¨¡å‹å­¦ä¹ åˆ°çš„æœ€å¼ºèŠ‚ç‚¹å…³ç³»(è®­ç»ƒå)")
print("="*80)

# æ¢å¤å¯¹è§’çº¿ä¸º0
np.fill_diagonal(cos_sim_after, 0)

# æ‰¾å‡ºTop10æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹å¯¹
triu_indices = np.triu_indices(NODE_NUM, k=1)
similarities = cos_sim_after[triu_indices]
top_indices = np.argsort(similarities)[::-1][:10]

print("\nå‰10ä¸ªæœ€ç›¸ä¼¼çš„èŠ‚ç‚¹å¯¹:")
for rank, idx in enumerate(top_indices):
    i, j = triu_indices[0][idx], triu_indices[1][idx]
    sim_after = cos_sim_after[i, j]
    sim_before = cosine_similarity(embeddings_before[i:i+1], embeddings_before[j:j+1])[0, 0]
    change = sim_after - sim_before
    
    print(f"{rank+1:2d}. èŠ‚ç‚¹{i:2d} â†” èŠ‚ç‚¹{j:2d}  |  "
          f"ç›¸ä¼¼åº¦: {sim_after:7.4f}  |  å˜åŒ–: {change:+.4f}")

print("\n" + "="*80)
print("âœ“ åˆ†æå®Œæˆ!")
print("="*80)
print("\nğŸ’¡ æ€»ç»“:")
print("   - è®­ç»ƒå‰çš„åµŒå…¥æ˜¯éšæœºåˆå§‹åŒ–çš„,èŠ‚ç‚¹é—´å…³ç³»ä¸æ˜ç¡®")
print("   - è®­ç»ƒåçš„åµŒå…¥æ˜¯æ¨¡å‹å­¦ä¹ åˆ°çš„,åæ˜ äº†çœŸå®çš„èŠ‚ç‚¹ç›¸å…³æ€§")
print("   - å¯è§†åŒ–å›¾ä¸­é è¿‘çš„èŠ‚ç‚¹=æ¨¡å‹è®¤ä¸ºå®ƒä»¬åœ¨å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­ç›¸å…³")
print("="*80)

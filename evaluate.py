"""
GDN è¯„ä¼°æ¨¡å—
ç”¨äºè®¡ç®—å¼‚å¸¸æ£€æµ‹çš„å„é¡¹è¯„ä¼°æŒ‡æ ‡
"""
from util.data import *
from util.debug_logger import get_logger
import numpy as np
from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score


def get_full_err_scores(test_result, val_result):
    """
    è®¡ç®—æ‰€æœ‰ç‰¹å¾çš„è¯¯å·®åˆ†æ•°
    
    Args:
        test_result: æµ‹è¯•ç»“æœ [predictions, ground_truth, labels]
        val_result: éªŒè¯ç»“æœ [predictions, ground_truth, labels]
    
    Returns:
        all_scores: æ‰€æœ‰ç‰¹å¾çš„è¯¯å·®åˆ†æ•° [feature_num, sample_num]
        all_normals: æ­£å¸¸åˆ†æ•°åˆ†å¸ƒ
    """
    logger = get_logger()
    
    np_test_result = np.array(test_result)
    np_val_result = np.array(val_result)

    all_scores =  None
    all_normals = None
    feature_num = np_test_result.shape[-1]

    labels = np_test_result[2, :, 0].tolist()
    
    logger.log_section("è¯¯å·®åˆ†æ•°è®¡ç®—", icon='ğŸ“ˆ')
    logger.log("ç‰¹å¾æ•°é‡", feature_num)
    logger.log("æµ‹è¯•æ ·æœ¬æ•°", np_test_result.shape[1])

    for i in range(feature_num):
        test_re_list = np_test_result[:2,:,i]
        val_re_list = np_val_result[:2,:,i]

        scores = get_err_scores(test_re_list, val_re_list)
        normal_dist = get_err_scores(val_re_list, val_re_list)
        
        # æ‰“å°å‰å‡ ä¸ªç‰¹å¾çš„è¯¯å·®ä¿¡æ¯
        if i < 3:
            logger.log(f"ç‰¹å¾{i}è¯¯å·®", f"èŒƒå›´ [{scores.min():.4f}, {scores.max():.4f}], å‡å€¼ {scores.mean():.4f}")

        if all_scores is None:
            all_scores = scores
            all_normals = normal_dist
        else:
            all_scores = np.vstack((
                all_scores,
                scores
            ))
            all_normals = np.vstack((
                all_normals,
                normal_dist
            ))
    
    logger.log("è¯¯å·®åˆ†æ•°çŸ©é˜µ", f"shape={all_scores.shape}")

    return all_scores, all_normals


def get_final_err_scores(test_result, val_result):
    """è·å–æœ€ç»ˆè¯¯å·®åˆ†æ•°ï¼ˆå–å„ç‰¹å¾æœ€å¤§å€¼ï¼‰"""
    full_scores, all_normals = get_full_err_scores(test_result, val_result, return_normal_scores=True)

    all_scores = np.max(full_scores, axis=0)

    return all_scores



def get_err_scores(test_res, val_res):
    """
    è®¡ç®—å•ä¸ªç‰¹å¾çš„è¯¯å·®åˆ†æ•°
    
    Args:
        test_res: [predictions, ground_truth]
        val_res: [predictions, ground_truth]
    
    Returns:
        smoothed_err_scores: å¹³æ»‘åçš„è¯¯å·®åˆ†æ•°
    """
    test_predict, test_gt = test_res
    val_predict, val_gt = val_res

    n_err_mid, n_err_iqr = get_err_median_and_iqr(test_predict, test_gt)

    test_delta = np.abs(np.subtract(
                        np.array(test_predict).astype(np.float64), 
                        np.array(test_gt).astype(np.float64)
                    ))
    epsilon=1e-2

    err_scores = (test_delta - n_err_mid) / ( np.abs(n_err_iqr) +epsilon)

    smoothed_err_scores = np.zeros(err_scores.shape)
    before_num = 3
    for i in range(before_num, len(err_scores)):
        smoothed_err_scores[i] = np.mean(err_scores[i-before_num:i+1])

    
    return smoothed_err_scores



def get_loss(predict, gt):
    """è®¡ç®—MSEæŸå¤±"""
    return eval_mseloss(predict, gt)

def get_f1_scores(total_err_scores, gt_labels, topk=1):
    """è®¡ç®—F1åˆ†æ•°"""
    print('total_err_scores', total_err_scores.shape)
    # remove the highest and lowest score at each timestep
    total_features = total_err_scores.shape[0]

    # topk_indices = np.argpartition(total_err_scores, range(total_features-1-topk, total_features-1), axis=0)[-topk-1:-1]
    topk_indices = np.argpartition(total_err_scores, range(total_features-topk-1, total_features), axis=0)[-topk:]
    
    topk_indices = np.transpose(topk_indices)

    total_topk_err_scores = []
    topk_err_score_map=[]
    # topk_anomaly_sensors = []

    for i, indexs in enumerate(topk_indices):
       
        sum_score = sum( score for k, score in enumerate(sorted([total_err_scores[index, i] for j, index in enumerate(indexs)])) )

        total_topk_err_scores.append(sum_score)

    final_topk_fmeas = eval_scores(total_topk_err_scores, gt_labels, 400)

    return final_topk_fmeas

def get_val_performance_data(total_err_scores, normal_scores, gt_labels, topk=1):
    """
    ä½¿ç”¨éªŒè¯é›†é˜ˆå€¼è®¡ç®—æ€§èƒ½
    
    Returns:
        f1, precision, recall, auc_score, threshold
    """
    logger = get_logger()
    
    total_features = total_err_scores.shape[0]

    topk_indices = np.argpartition(total_err_scores, range(total_features-topk-1, total_features), axis=0)[-topk:]

    total_topk_err_scores = []
    topk_err_score_map=[]

    total_topk_err_scores = np.sum(np.take_along_axis(total_err_scores, topk_indices, axis=0), axis=0)

    thresold = np.max(normal_scores)
    
    logger.log_subsection("éªŒè¯é›†é˜ˆå€¼è¯„ä¼°", icon='ğŸ“Š')
    logger.log("é˜ˆå€¼æ¥æº", "éªŒè¯é›†æœ€å¤§è¯¯å·®åˆ†æ•°")
    logger.log("é˜ˆå€¼", f"{thresold:.4f}")

    pred_labels = np.zeros(len(total_topk_err_scores))
    pred_labels[total_topk_err_scores > thresold] = 1

    for i in range(len(pred_labels)):
        pred_labels[i] = int(pred_labels[i])
        gt_labels[i] = int(gt_labels[i])

    pre = precision_score(gt_labels, pred_labels)
    rec = recall_score(gt_labels, pred_labels)

    f1 = f1_score(gt_labels, pred_labels)


    auc_score = roc_auc_score(gt_labels, total_topk_err_scores)
    
    # æ‰“å°è¯¦ç»†è¯„ä¼°ç»“æœ
    n_pred_anomaly = int(pred_labels.sum())
    n_gt_anomaly = int(sum(gt_labels))
    logger.log("é¢„æµ‹å¼‚å¸¸æ•°", f"{n_pred_anomaly} / {len(pred_labels)}")
    logger.log("çœŸå®å¼‚å¸¸æ•°", f"{n_gt_anomaly} / {len(gt_labels)}")

    return f1, pre, rec, auc_score, thresold


def get_best_performance_data(total_err_scores, gt_labels, topk=1):
    """
    æœç´¢æœ€ä¼˜é˜ˆå€¼è®¡ç®—æœ€ä½³æ€§èƒ½
    
    Returns:
        best_f1, precision, recall, auc_score, threshold
    """
    logger = get_logger()

    total_features = total_err_scores.shape[0]

    # topk_indices = np.argpartition(total_err_scores, range(total_features-1-topk, total_features-1), axis=0)[-topk-1:-1]
    topk_indices = np.argpartition(total_err_scores, range(total_features-topk-1, total_features), axis=0)[-topk:]

    total_topk_err_scores = []
    topk_err_score_map=[]

    total_topk_err_scores = np.sum(np.take_along_axis(total_err_scores, topk_indices, axis=0), axis=0)
    
    logger.log_subsection("æœ€ä¼˜é˜ˆå€¼æœç´¢", icon='ğŸ”')
    logger.log("æœç´¢æ­¥æ•°", 400)

    final_topk_fmeas ,thresolds = eval_scores(total_topk_err_scores, gt_labels, 400, return_thresold=True)

    th_i = final_topk_fmeas.index(max(final_topk_fmeas))
    thresold = thresolds[th_i]
    
    logger.log("æœ€ä¼˜é˜ˆå€¼ä½ç½®", f"ç¬¬ {th_i + 1} / 400 æ­¥")
    logger.log("æœ€ä¼˜é˜ˆå€¼", f"{thresold:.4f}")

    pred_labels = np.zeros(len(total_topk_err_scores))
    pred_labels[total_topk_err_scores > thresold] = 1

    for i in range(len(pred_labels)):
        pred_labels[i] = int(pred_labels[i])
        gt_labels[i] = int(gt_labels[i])

    pre = precision_score(gt_labels, pred_labels)
    rec = recall_score(gt_labels, pred_labels)

    auc_score = roc_auc_score(gt_labels, total_topk_err_scores)
    
    # æ‰“å°å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ
    normal_mask = np.array(gt_labels) == 0
    anomaly_mask = np.array(gt_labels) == 1
    
    if sum(normal_mask) > 0 and sum(anomaly_mask) > 0:
        normal_scores = total_topk_err_scores[normal_mask]
        anomaly_scores = total_topk_err_scores[anomaly_mask]
        
        logger.log_subsection("å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ", icon='ğŸ“‰')
        logger.log("æ­£å¸¸æ ·æœ¬åˆ†æ•°", f"mean={normal_scores.mean():.4f}, std={normal_scores.std():.4f}, max={normal_scores.max():.4f}")
        logger.log("å¼‚å¸¸æ ·æœ¬åˆ†æ•°", f"mean={anomaly_scores.mean():.4f}, std={anomaly_scores.std():.4f}, max={anomaly_scores.max():.4f}")
        
        # è®¡ç®—åˆ†ç¦»åº¦
        separation = (anomaly_scores.mean() - normal_scores.mean()) / (normal_scores.std() + 1e-8)
        logger.log("åˆ†ç¦»åº¦", f"{separation:.4f} (è¶Šå¤§è¶Šå¥½)")

    return max(final_topk_fmeas), pre, rec, auc_score, thresold
